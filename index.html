<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
  * {
    box-sizing: border-box;
  }

	body {
		background: radial-gradient(circle at top left, #4DA167 0%, #4E6E5D 30%, #4D5057 100%);
		color: #CFCFCF;
		margin: 0;
		padding: 40px 0;
		font-family: "Inter", -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	/* Make sure this selector has the dot; yours above was missing it */
	.content-section {
		margin: 3rem auto;
		max-width: 900px;
		padding: 0 1.25rem;
	}
	/* Reduce or increase the gap above the very first section */
.content-section:first-of-type {
  margin-top: 1.5rem;  /* tweak this up/down to taste */
}

/* Dark green cards with white Garamond text */
	.content-card {
  background: linear-gradient(
    225deg,
    #0c1d14 0%,    /* slightly darker than your base */
    #102a1c 40%,   /* your existing dark green */
    #163528 100%   /* slightly lighter, still in palette */
  );
  color: #ffffff;
  border-radius: 10px;
  border: 1px solid #1d4630;
  padding: 1.5rem 1.75rem;
  box-shadow: 0 10px 25px rgba(0, 0, 0, 0.25);
  line-height: 1.6;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica,
               Arial, sans-serif;
}
/* Ensure card can host an overlay */
.content-card {
  position: relative;
  overflow: hidden;
}

/* Radial glow that follows the cursor */
.content-card::before {
  content: "";
  position: absolute;
  inset: -120px; /* extend beyond edges so glow doesn't clip */
  pointer-events: none;
  background: radial-gradient(
    circle at var(--glow-x, 50%) var(--glow-y, 50%),
    rgba(120, 255, 180, 0.26),   /* inner light green */
    rgba(120, 255, 180, 0.08) 20%,
    transparent 55%
  );
  opacity: 0;
  transition: opacity 0.2s ease-out;
  z-index: 0;
}

/* Turn glow on when hovered */
.content-card:hover::before {
  opacity: 1;
}

/* Keep text above the glow */
.content-card > * {
  position: relative;
  z-index: 1;
}

		/* Ensure lists match the card text styling */
	.content-card p + p {
		margin-top: 0.9rem;
	}

	.content-card ul {
		margin: 0.75rem 0 0.75rem 1.2rem;
		padding-left: 1.1rem;
	}

	.content-card li {
		margin-bottom: 0.4rem;
	}

		/* Optional: emphasize strong text but keep it white */
	.content-card li strong {
		font-weight: 600;
		color: #ffffff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: center;
		align-items: flex-start;
		margin-bottom: 32px;
	}
	.content-section h2 {
		position: relative;
		padding-left: 2.3rem;  /* makes room for the note */
		font-size: 1.8rem;
		margin-bottom: 1rem;
		letter-spacing: 0.02em;
	}

	/* White music note to the left of each section title */
	.content-section h2::before {
	content: "♪";
	position: absolute;
	left: -0.6rem;          /* offset a bit toward the black box edge */
	top: 50%;
	transform: translateY(-50%);
	font-size: 1.5rem;
	color: #ffffff;
	opacity: 0.9;
	pointer-events: none;
	}

	.main-content-block {
  width: 70%;
  max-width: 1100px;
  background-color: rgba(13, 16, 23, 0.96);
  border-radius: 18px;
  padding: 24px 32px;
  box-shadow:
    0 18px 80px rgba(0,0,0,0.65),
    0 0 0 1px rgba(77,80,87,0.5);
  backdrop-filter: blur(18px);
  color: #E6E6E6;
  position: relative;
  z-index: 1;
  margin-left: auto;
  margin-right: auto;
}

	.margin-left-block {
		font-size: 14px;
		width: 15%;
		max-width: 130px;
		position: relative;
		margin-left: 10px;
		text-align: left;
		font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		padding: 5px;
		color: #CFCFCF;
	}

	.margin-right-block {
		font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		font-size: 14px;
		width: 25%;
		max-width: 256px;
		position: relative;
		text-align: left;
		padding: 10px;
		color: #CFCFCF;
	}

	img {
		max-width: 100%;
		height: auto;
		display: block;
		margin: auto;
		border-radius: 14px;
		box-shadow: 0 0 0 rgba(0,0,0,0);
		transform: translateY(0) scale(1);
		transition:
			transform 280ms ease,
			box-shadow 280ms ease,
			filter 280ms ease;
	}

	.my-video {
		max-width: 100%;
		height: auto;
		display: block;
		margin: auto;
		border-radius: 18px;
		box-shadow: 0 0 0 rgba(0,0,0,0);
		transform: translateY(0) scale(1);
		transition:
			transform 280ms ease,
			box-shadow 280ms ease,
			filter 280ms ease;
	}

  /* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	/* Image & video hover: deep “shadow in the background” */
	img:hover,
	.my-video:hover {
		transform: translateY(-6px) scale(1.02);
		box-shadow:
			0 30px 80px rgba(0, 0, 0, 0.85),
			0 0 0 1px rgba(207,207,207,0.25);
		filter: drop-shadow(0 24px 60px rgba(61, 247, 114, 0.45));
	}

	a:link,
	a:visited {
		color: #3BC14A;
		text-decoration: none;
		font-weight: 500;
		transition: color 180ms ease, text-shadow 180ms ease;
	}

	a:hover {
		color: #4DA167;
		text-shadow: 0 0 12px rgba(59,193,74,0.7);
	}

	h1 {
		font-size: 24px;
		margin-top: 4px;
		margin-bottom: 14px;
		color: #FFFFFF;
		letter-spacing: 0.02em;
	}

	table.header {
		font-weight: 400;
		font-size: 18px;
		flex-grow: 1;
		width: 100%;
		max-width: 100%;
		color: #FFFFFF;
	}

	.paper-title {
		display: block;
		font-family: "Poppins", system-ui, -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		font-size: 32px;
		font-weight: 600;
		letter-spacing: 0.03em;
		color: #FFFFFF;
		margin-bottom: 18px; /* controls space above author names */
		}

	.paper-authors {
		font-family: "Inter", -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		font-size: 17px;
		font-weight: 400;
		color: #CFCFCF;
	}

	.paper-subtitle {
		font-family: "Inter", -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		font-size: 16px;
		color: #A8B0BA;
		padding-top: 8px;
	}

	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35),
		        5px 5px 0 0px #0D1017,
		        5px 5px 1px 1px rgba(0,0,0,0.35),
		        10px 10px 0 0px #0D1017,
		        10px 10px 1px 1px rgba(0,0,0,0.35);
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
		border-radius: 16px;
	}

	hr {
    height: 1px;
    border: none;
    background: linear-gradient(to right, rgba(77,80,87,0), rgba(207,207,207,0.7), rgba(77,80,87,0));
  }

	div.hypothesis {
		width: 80%;
		background: linear-gradient(135deg, #4E6E5D 0%, #4DA167 100%);
		border: 1px solid rgba(207,207,207,0.6);
		border-radius: 16px;
		font-family: "SF Mono", Menlo, Monaco, Consolas, "Courier New", Courier, monospace;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
		color: #0D1017;
		box-shadow: 0 14px 40px rgba(0,0,0,0.5);
	}

	div.citation {
    font-size: 0.9em;
    background-color:#141820;
    padding: 14px;
		height: 200px;
		border-radius: 12px;
		border: 1px solid rgba(207,207,207,0.2);
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block;
	    vertical-align: top;
	    width: 50px;
	}
	.getmusic-figures-row {
  display: flex;
  gap: 1.5rem;
  justify-content: center;
  align-items: flex-start;
  margin: 1.25rem 0 0.75rem;
}

.getmusic-figure {
  flex: 1 1 0;
  margin: 0;
}

.getmusic-figure img {
  width: 100%;
  height: auto;
  border-radius: 8px;
  display: block;
}
	.paper-authors {
  display: flex;
  align-items: baseline;
  gap: 0.7rem;        /* was 1.75rem */
  font-size: 0.95rem;
}

.authors-label {
  text-transform: none;       /* keep normal capitalization */
  letter-spacing: 0;          /* remove extra spacing between letters */
  font-weight: 400;           /* lighter weight */
  opacity: 0.8;               /* slightly dimmer than names */
}

.author-name {
  text-decoration: none;
}
	body {
  position: relative;
  overflow-x: hidden; /* avoid horizontal scroll from the animation */
}

html::before {
  content: "♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬";
  position: fixed;
  top: 8%;
  left: -40%;
  width: 260%;
  pointer-events: none;
  font-size: 52px;
  color: rgba(0, 0, 0, 0.22);
  white-space: nowrap;
  z-index: 0;
  animation: float-notes-1 45s linear infinite;
}

/* ===== Layer 2: mid, right-to-left ===== */
html::after {
  content: "♬ ♩ ♪ ♫ ♩ ♬ ♫ ♪ ♩ ♬ ♫ ♪ ♩ ♬ ♫";
  position: fixed;
  top: 42%;
  left: -60%;
  width: 280%;
  pointer-events: none;
  font-size: 64px;
  color: rgba(0, 0, 0, 0.28);
  white-space: nowrap;
  z-index: 0;
  animation: float-notes-2 55s linear infinite;
}

/* ===== Layer 3: lower, left-to-right ===== */
body::before {
  content: "♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬";
  position: fixed;
  bottom: 20%;
  left: -50%;
  width: 260%;
  pointer-events: none;
  font-size: 48px;
  color: rgba(0, 0, 0, 0.20);
  white-space: nowrap;
  z-index: 0;
  animation: float-notes-3 35s linear infinite;
}

/* ===== Layer 4: very low, right-to-left ===== */
body::after {
  content: "♬ ♩ ♪ ♫ ♩ ♬ ♫ ♪ ♩ ♬ ♫ ♪ ♩ ♬ ♫";
  position: fixed;
  bottom: 5%;
  left: -70%;
  width: 300%;
  pointer-events: none;
  font-size: 72px;
  color: rgba(0, 0, 0, 0.18);
  white-space: nowrap;
  z-index: 0;
  animation: float-notes-4 65s linear infinite;
}

/* Keyframes for each strip */
@keyframes float-notes-1 {
  0%   { transform: translateX(0); }
  100% { transform: translateX(40%); }
}

@keyframes float-notes-2 {
  0%   { transform: translateX(0); }
  100% { transform: translateX(-50%); }
}

@keyframes float-notes-3 {
  0%   { transform: translateX(0); }
  100% { transform: translateX(35%); }
}

@keyframes float-notes-4 {
  0%   { transform: translateX(0); }
  100% { transform: translateX(-60%); }
}
	@keyframes sweepMask {
		0% { -webkit-mask-position: 0% 0; mask-position: 0% 0; }
		100% { -webkit-mask-position: 100% 0; mask-position: 100% 0; }
	}

	/* Animated button styles */
	.btn {
		display: inline-flex;
		align-items: center;
		justify-content: center;
		padding: 10px 20px;
		border-radius: 999px;
		border: 1px solid rgba(207,207,207,0.5);
		background: radial-gradient(circle at top left, #4DA167, #3BC14A);
		color: #0D1017;
		font-size: 14px;
		font-weight: 600;
		letter-spacing: 0.06em;
		text-transform: uppercase;
		cursor: pointer;
		box-shadow:
			0 10px 25px rgba(0,0,0,0.6),
			0 0 0 1px rgba(0,0,0,0.9);
		position: relative;
		overflow: hidden;
		transition:
			transform 160ms ease,
			box-shadow 160ms ease,
			background 200ms ease,
			border-color 160ms ease;
	}

	.btn:hover {
		transform: translateY(-2px) scale(1.02);
		box-shadow:
			0 18px 40px rgba(0,0,0,0.8),
			0 0 0 1px rgba(207,207,207,0.8);
		background: radial-gradient(circle at top left, #4E6E5D, #4DA167);
		border-color: #CFCFCF;
	}

	.btn:active {
		transform: translateY(1px) scale(0.98);
		box-shadow:
			0 8px 18px rgba(0,0,0,0.9),
			0 0 0 1px rgba(207,207,207,0.6);
	}

	.btn::after {
		content: "";
		position: absolute;
		inset: 0;
		background: radial-gradient(circle at center, rgba(255,255,255,0.7) 0%, transparent 60%);
		opacity: 0;
		transform: scale(0);
		transition: opacity 260ms ease, transform 260ms ease;
		pointer-events: none;
		mix-blend-mode: screen;
	}

	.btn:active::after {
		opacity: 1;
		transform: scale(1.6);
	}
</style>

	  <title>The Platonic Representation Hypothesis</title>
      <meta property="og:title" content="The Platonic Representation Hypothesis" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align="left">
							<tr>
								<td colspan="4">
								<span class="paper-title">
									Music-Theory Informed Loss for Polyphonic MIDI Generation with Transformer Diffusion Models
								</span>
								</td>
							</tr>
							<tr>
								<td align="left" colspan="4" class="paper-authors">
  <span class="authors-label">Authors:</span>
  <a class="author-name" href="https://github.com/mihikadusad">Mihika Dusad</a>
  <a class="author-name" href="https://github.com/victoriaxhu">Victoria Hu</a>
</td>
							</tr>
							<tr>
								<td colspan="4" align="left">
								<span class="paper-subtitle">Final project for 6.7960, MIT</span>
								</td>
							</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#does_x_do_y">Does X do Y?</a><br><br>
              <a href="#implications_and_limitations">Implications and limitations</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
			<!-- ===================== Introduction ===================== -->
<section id="introduction" class="content-section">
  <h2>Introduction</h2>
  <div class="content-card">
    <p class="introduction-content-section">
      Symbolic music generation refers to creating written music — notes, rhythms, instruments, chords — rather than audio waveforms. Instead of generating sound directly, models work with MIDI (Musical Instrument Digital Interface), a digital score format that encodes musical events such as pitch, duration, timing, and instrument choice. Symbolic generation often focuses on polyphonic music, where multiple notes or musical lines sound simultaneously, such as in piano pieces, string quartets, or multi-track arrangements.
    </p>
    <p>
      Most current systems represent MIDI as a single sequence of note events, forcing the model to implicitly discover 2D structure — melodic continuity, chord consistency, and voice independence — from flattened data. Despite improvements in tokenization schemes, these models often struggle to maintain stable voice lines or valid harmonic textures. In reality, polyphonic music is inherently two-dimensional: the horizontal axis reflects how melodies evolve over time, while the vertical axis captures the harmony formed by notes sounding together at each moment. Flattening this grid into a 1D sequence obscures both dimensions, making it significantly harder for models to learn coherent melodic motion and consistent chordal structure.
    </p>
    <p>
      GETMusic, a framework developed by Microsoft Research, takes a different approach by converting MIDI into a 2D track–time image-like representation, enabling a diffusion-based transformer to model local and global musical patterns directly in two dimensions. This representation introduces a stronger inductive bias than single-stream event models, since vertical and horizontal relationships are spatially explicit rather than reconstructed from a token sequence. However, harmony in GETMusic remains an emergent property: the diffusion objective is a generic reconstruction loss that does not encode any explicit music-theoretic principles. As a result, GETMusic remains heavily dependent on extremely large MIDI datasets, and the original authors invested substantial effort in crawling and cleaning thousands of hours of symbolic music to obtain sufficient training data.
    </p>
	 <div class="getmusic-figures-row">
  <figure class="getmusic-figure">
    <img src="./images/getmusic1_1.png" alt="GETMusic representation example 1">
  </figure>
  <figure class="getmusic-figure">
    <img src="./images/getmusic1_2.png" alt="GETMusic representation example 2">
  </figure>
</div>
    <p>
      Our contribution is to build upon GETMusic by incorporating an explicit, music-theory-informed loss regularization term into the GETMusic framework to more reliably enforce vertical harmonic constraints. By aligning the objective function with known principles of polyphonic writing, we strengthen the inductive biases already present in GETMusic’s 2D representation and reduce the dependence on massive curated datasets. We aim to produce more coherent polyphonic outcomes, especially when training resources or datasets are limited.
    </p>
  </div>
</section>

<!-- = History of sequence-based symbolic MIDI generation = -->
<section id="history-sequence" class="content-section">
  <h2>History of sequence-based symbolic MIDI generation and their limitations</h2>
  <div class="content-card">
    <p>
      Early neural approaches to symbolic music generation modeled notes as time-ordered sequences, typically using RNNs to generate monophonic or lightly polyphonic textures (Boulanger-Lewandowski et al., 2012). These models struggled with long-range structure, voice independence, and harmonic consistency. Later, transformer-based systems improved long-range coherence, but virtually all popular models still rely on flattened 1D event streams, leaving it to the architecture to reconstruct multi-voice structure implicitly.
    </p>
    <p>Notable examples include:</p>
    <ul>
      <li>
        <strong>Music Transformer (Huang et al., 2018)</strong>: Introduces relative positional self-attention to capture long-term structure, but still represents all notes as a single interleaved 1D event stream with no explicit vertical alignment.
      </li>
      <li>
        <strong>REMI (Pop Music Transformer) (Huang &amp; Yang, 2020)</strong>: Adds musically meaningful meta-events (bars, beats, chords, tempo) to improve structural awareness, yet ultimately remains a purely sequential token representation.
      </li>
      <li>
        <strong>Compound Word Transformer (CP-Transformer) (Hsiao et al., 2021)</strong>: Groups event attributes (pitch, duration, velocity, instrument for a single note) into “compound tokens” rather than letting these attributes be separate tokens, but the model still processes a 1D stream of notes without dedicated structure for vertical chords.
      </li>
      <li>
        <strong>PopMAG (et al., 2020)</strong>: Generates multi-track pop arrangements using track embeddings to indicate instrument identity, while still relying on a unified event sequence that leaves cross-track harmony implicit.
      </li>
      <li>
        <strong>MMM (Multi-Track Music Machine) (Ens &amp; Pasquier, 2020)</strong>: For multi-track music, the representation concatenates the tracks; despite the “multi-track” name, the underlying representation is 1D.
      </li>
      <li>
        <strong>MTMT (Multitrack Music Transformer) (Dong et al., 2023)</strong>: Proposes a transformer architecture for multi-track symbolic music generation by encoding each musical event as a tuple of attributes (instrument, pitch, duration, velocity, etc.), yet ultimately serializes all tracks into one long event sequence.
      </li>
      <li>
        <strong>Nested Music Transformer (Ryu et al., 2024)</strong>: Encodes event tuples (type, beat, pitch, duration, instrument), but fully flattens them into a time-ordered sequence.
      </li>
    </ul>
    <p>
      Across all of these, track or voice identity is encoded as a token attribute rather than a structural axis. As a result, vertical harmony emerges only from learned attention weights rather than from architectural bias, making the models prone to voice crossing, unstable chord structures, inconsistent spacing, and low-level harmonic errors unless trained on extremely large datasets. This limitation motivates approaches that retain the benefits of transformers while embedding stronger structural inductive biases.
    </p>
  </div>
</section>

<!-- = Non-transformer models with explicit 2D inductive biases = -->
<section id="non-transformer-2d" class="content-section">
  <h2>Non-transformer models with explicit 2D inductive biases</h2>
  <div class="content-card">
    <p>
      Prior to the Transformer era, several models demonstrated that explicitly modeling the 2D music grid (voices × time) produces more coherent voice-leading and harmony:
    </p>
    <ul>
      <li>
        <strong>DeepBach (Hadjeres et al., 2017)</strong>: Models chorales as a matrix of pitches over discrete time steps; the sampling process treats vertical slices of the score as first-class objects.
      </li>
      <li>
        <strong>Coconet / Counterpoint by Convolution (Huang et al., 2019)</strong>: Uses a large convolutional network over a 2D piano-roll representation (pitch × time) to enable inpainting and harmony generation.
      </li>
    </ul>
    <p>
      These systems succeed largely because they directly encode vertical relationships, making correct chord structures and spacing easier for the network to learn. However, they are largely restricted to four-part choral textures and do not readily generalize to multi-track MIDI with arbitrary instrumentation, expressive timing, or long pieces. Moreover, they lack the generative flexibility and scalability of modern diffusion or transformer models.
    </p>
  </div>
</section>

<!-- = Choosing GETMusic as a baseline: MIDI score to image representation = -->
<section id="getmusic-baseline" class="content-section">
  <h2>Choosing GETMusic as a baseline: MIDI score to image representation</h2>
  <div class="content-card">
    <p>
      GETMusic combines a 2D symbolic representation with diffusion-based generation, bridging the gap between classical 2D harmony models and modern deep generative architectures. Rather than representing MIDI as a sequence of events, GETMusic rasterizes symbolic music into a pitch × time grid representation known as GETScore. This creates a multi-channel “image” where horizontal continuity corresponds to melodic flow and vertical stacks correspond to chords or polyphonic textures.
    </p>
    <p>
      GETMusic applies a discrete diffusion model, GETDiff, to the GETScore representation to progressively reconstruct clean symbolic tracks from masked, corrupted inputs. The denoiser is implemented as a transformer with RoFormer layers: the model embeds the 2D GETScore grid (tracks arranged vertically, time units horizontally, with separate rows for pitch and duration per track), adds learnable condition flags that indicate which tokens are sources/targets/ignored, projects into a model dimension, and then feeds the sequence through 12 RoFormer layers followed by a classification head over the token vocabulary.
    </p>
    <p>
      In the forward (diffusion) process, target-track tokens are iteratively corrupted by transitioning them toward a special mask token according to a categorical Markov chain, while source-track tokens are kept as ground truth and uninvolved tracks are filled with a special empty token that does not diffuse. In the denoising process, at each timestep GETDiff predicts the clean token distribution for all positions non-autoregressively, i.e., it generates all tokens in parallel conditioned on the current noisy GETScore, the source tracks, and condition flags; after each reverse step, source tracks are reset to their ground truth values and uninvolved tracks are re-emptied.
    </p>
    <p>
      By randomly sampling source and target tracks during training, the same model can handle any source-target combination without retraining. In this case, there are tracks for lead melody, piano, guitar, strings, bass, and drums. GETScore’s vertically stacked, temporally aligned tracks preserve interdependencies among simultaneous notes within and across tracks, so GETDiff always denoises in a space where track separations and time alignment are explicit.
    </p>
  </div>
</section>

<!-- = Architectural limitations = -->
<section id="architectural-limitations" class="content-section">
  <h2>Architectural limitations</h2>
  <div class="content-card">
    <p>
      GETScore and GETDiff together introduce a stronger structural bias than standard sequence-based models: tracks are explicitly separated yet temporally aligned, and within-track simultaneity is compacted into compound pitch tokens derived from real data. However, from an optimization perspective, GETMusic is still trained with a generic discrete diffusion objective over tokens: the loss is a variational lower bound with categorical transitions and an additional x₀-parameterization term, effectively encouraging the model to reconstruct the original GETScore tokens from noisy versions.
    </p>
    <p>
      Crucially, this objective does not encode any explicit music-theoretic constraints. In particular, GETMusic does not directly model or penalize chord plausibility or spacing (beyond what is implicit in the data), or stylistic consistency of harmony or voice leading. Harmony and voice leading are therefore emergent properties that the model must infer implicitly from the training corpus, rather than being enforced by the loss. This interacts with the inherent data demands of diffusion: GETDiff is trained with 100 denoising steps over a large vocabulary (11,883 tokens) and a 2D multi-track grid, so learning robust harmonic behavior requires substantial coverage of stylistic and structural patterns. In practice, the authors mitigate this by crawling 1.56M MIDI files from MuseScore and constructing 137,812 GETScores (approximately 2,800 hours worth) after strict cleaning and preprocessing, but assembling such a dataset and pipeline is nontrivial for the average person.
    </p>
    <p>
      These factors motivate our approach: rather than relying solely on large-scale data and a generic categorical diffusion objective to “discover” harmonic structure, we introduce music-theory-informed regularization terms that explicitly favor vertically coherent harmony within the GETMusic framework.
    </p>
  </div>
</section>

<!-- = Adding explicit music-theory-informed loss regularization term = -->
<section id="loss-regularization" class="content-section">
  <h2>Adding explicit music-theory-informed loss regularization term</h2>
  <div class="content-card">
    <p><em>(Content to be added.)</em></p>
  </div>
</section>

<!-- = Our methodology = -->
<section id="methodology" class="content-section">
  <h2>Our methodology</h2>
  <div class="content-card">
    <p><em>(Content to be added.)</em></p>
  </div>
</section>
			<div class="project-description">
  <div class="project-description">
</div>
</div>
		</div>
		    <div class="margin-right-block">
						
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
            Motivate your project. What question are you asking. Why is it unanswered so far? What gap in the literature or practice are you filling?
            Why is it important?
		    </div>
		    <div class="margin-right-block">
						Margin note that clarifies some detail #main-content-block for intro section.
		    </div>
		</div>

		<div class="content-margin-container" id="does_x_do_y">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Does X do Y?</h1>
				  It is well known that Y does Y. And this has raised the question does X do Y? Because if Y does Y then it stands to reason that X does Y.
          But we cannot answer this until we realize the Z implies Y and X can be linked to Z.<br><br>

          Now let's write some math!<br>
          <center>
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <mi>y</mi>
                </mrow>
                <mo>/</mo>
                <mrow>
                  <mo>&#x2202;</mo>
                  <mi>x</mi>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mi>x</mi>
            </math>
          </center>
          <br>
          It's probably best to ask an LLM to help do the web
          formatting for math. You can tell it "convert this latex equation into MathML: $$\frac{\partial dy}{\partial dx} = x$$"
          But it took me a few tries. So, if you get frustrated, you can embed an image of the equation, or use other packages for
          rendering equations on webpages.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Another section</h1>
            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		    <div class="margin-right-block">
					A caption for the video could go here.
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and limitations</h1>
						Let's end with some discussion of the implications and limitations.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
							<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>
	<script>
  document.querySelectorAll('.content-card').forEach(function(card) {
    card.addEventListener('mousemove', function(e) {
      const rect = card.getBoundingClientRect();
      const x = e.clientX - rect.left;
      const y = e.clientY - rect.top;
      card.style.setProperty('--glow-x', x + 'px');
      card.style.setProperty('--glow-y', y + 'px');
    });

    card.addEventListener('mouseleave', function() {
      // Optional: fade glow back to center when leaving
      card.style.removeProperty('--glow-x');
      card.style.removeProperty('--glow-y');
    });
  });
</script>
	</body>

</html>
