<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
  * {
    box-sizing: border-box;
  }

	body {
		background: radial-gradient(circle at top left, #f3f4f6 0%, #d6d8dc 40%, #b3b6bb 100%);
		color: #CFCFCF;
		margin: 0;
		padding: 40px 0;
		font-family: "Inter", -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	/* Make sure this selector has the dot; yours above was missing it */
	.content-section {
		margin: 3rem auto;
		max-width: 900px;
		padding: 0 1.25rem;
	}
	/* Reduce or increase the gap above the very first section */
.content-section:first-of-type {
  margin-top: 1.5rem;  /* tweak this up/down to taste */
}

/* White cards with black text */
	.content-card {
	  background: #ffffff;
	  color: #000000;
	  border-radius: 10px;
	  border: 1px solid #000000;
	  padding: 1.5rem 1.75rem;
	  box-shadow: 0 10px 25px rgba(0, 0, 0, 0.25);
	  line-height: 1.6;
	  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica,
	               Arial, sans-serif;
	}
/* Ensure card can host an overlay */
.content-card {
  position: relative;
  overflow: hidden;
}

/* Disable previous radial glow overlay on cards */
.content-card::before {
	content: none;
}

		/* Ensure lists match the card text styling */
	.content-card p + p {
		margin-top: 0.9rem;
	}

	.content-card ul {
		margin: 0.75rem 0 0.75rem 1.2rem;
		padding-left: 1.1rem;
	}

	.content-card li {
		margin-bottom: 0.4rem;
	}

		/* Optional: emphasize strong text but keep it black */
	.content-card li strong {
		font-weight: 600;
		color: #000000;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: center;
		align-items: flex-start;
		margin-bottom: 32px;
	}
	.content-section h2 {
		position: relative;
		padding-left: 2.3rem;  /* makes room for the note */
		font-size: 1.8rem;
		margin-bottom: 1rem;
		letter-spacing: 0.02em;
	}

	/* White music note to the left of each section title */
	.content-section h2::before {
	content: "♪";
	position: absolute;
	left: -0.6rem;          /* offset a bit toward the black box edge */
	top: 50%;
	transform: translateY(-50%);
	font-size: 1.5rem;
	color: #000000;
	opacity: 0.9;
	pointer-events: none;
	}

	.main-content-block {
	  width: 70%;
	  max-width: 1100px;
	  background-color: #ffffff;
	  border-radius: 18px;
	  padding: 24px 32px;
	  box-shadow:
	    0 14px 50px rgba(0,0,0,0.30);
	  backdrop-filter: blur(18px);
	  color: #000000;
	  position: relative;
	  z-index: 1;
	  margin-left: auto;
	  margin-right: auto;
	}

	.margin-left-block {
		font-size: 14px;
		width: 15%;
		max-width: 130px;
		position: sticky;
		margin-top: 16px;
		margin-left: 10px;
		text-align: left;
		font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		padding: 5px;
		color: #000000;
	}

	.margin-right-block {
		font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		font-size: 14px;
		width: 25%;
		max-width: 256px;
		position: relative;
		text-align: left;
		padding: 10px;
		color: #000000;
	}

	img {
		max-width: 100%;
		height: auto;
		display: block;
		margin: auto;
		border-radius: 14px;
		box-shadow: 0 0 0 rgba(0,0,0,0);
		transform: translateY(0) scale(1);
		transition:
			transform 280ms ease,
			box-shadow 280ms ease,
			filter 280ms ease;
	}

	.my-video {
		max-width: 100%;
		height: auto;
		display: block;
		margin: auto;
		border-radius: 18px;
		box-shadow: 0 0 0 rgba(0,0,0,0);
		transform: translateY(0) scale(1);
		transition:
			transform 280ms ease,
			box-shadow 280ms ease,
			filter 280ms ease;
	}

  /* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	/* Image & video hover: subtle gray shadow */
	img:hover,
	.my-video:hover {
		transform: translateY(-6px) scale(1.02);
		box-shadow:
			0 8px 20px rgba(0, 0, 0, 0.18),
			0 0 0 1px rgba(207,207,207,0.18);
		filter: drop-shadow(0 6px 16px rgba(0, 0, 0, 0.18));
	}

	a:link,
	a:visited {
		color: #000000;
		text-decoration: none;
		font-weight: 500;
		transition: color 180ms ease, text-shadow 180ms ease;
	}

	a:hover {
		color: #000000;
		text-shadow: none;
	}

	h1 {
		font-size: 24px;
		margin-top: 4px;
		margin-bottom: 14px;
		color: #000000;
		letter-spacing: 0.02em;
	}

	table.header {
		font-weight: 400;
		font-size: 18px;
		flex-grow: 1;
		width: 100%;
		max-width: 100%;
		color: #000000;
	}

	.paper-title {
		display: block;
		font-family: "Poppins", system-ui, -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		font-size: 32px;
		font-weight: 600;
		letter-spacing: 0.03em;
		color: #000000;
		margin-bottom: 18px; /* controls space above author names */
		background-color: #FFFFFF;
	}

	.paper-authors {
		font-family: "Inter", -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		font-size: 17px;
		font-weight: 400;
		color: #000000;
	}

	.paper-subtitle {
		font-family: "Inter", -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
		font-size: 16px;
		color: #000000;
		padding-top: 8px;
	}

	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35),
		        5px 5px 0 0px #0D1017,
		        5px 5px 1px 1px rgba(0,0,0,0.55);
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
		border-radius: 16px;
	}

	hr {
    height: 1px;
    border: none;
    background: linear-gradient(to right, rgba(77,80,87,0), rgba(207,207,207,0.7), rgba(77,80,87,0));
  }

	div.hypothesis {
		width: 80%;
		background: linear-gradient(135deg, #eef2f0 0%, #737672e9 100%);
		border: 1px solid rgba(207,207,207,0.6);
		border-radius: 16px;
		font-family: "SF Mono", Menlo, Monaco, Consolas, "Courier New", Courier, monospace;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
		color: #0D1017;
		box-shadow: 0 14px 40px rgba(0,0,0,0.5);
	}

	div.citation {
    font-size: 0.9em;
    background-color:#ffffff;
    padding: 14px;
		height: 200px;
		border-radius: 12px;
		border: 1px solid rgba(207,207,207,0.2);
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block;
	    vertical-align: top;
	    width: 50px;
	}
	.outline-title {
  font-size: 1.1rem;      /* larger than body text */
  font-weight: 600;
  color: black;
  letter-spacing: 0.02em;
  margin-bottom: 0.75rem;
}

.outline-links a {
  display: block;
  margin-bottom: 0.25rem;
  font-size: 0.7rem;        /* closer to normal text */
  color: black;            /* a bit softer green */
  text-decoration: none;
  padding: 0.05rem 0;
  border-radius: 3px;
  transition:
    color 0.15s ease,
    background-color 0.15s ease,
    transform 0.15s ease;
}

/* Light green emphasis on hover/active */
.outline-links a:hover {
  color: #21222225;        /* light green text on hover */
  background-color: transparent;   /* no dark green box */
  transform: translateX(1px);  /* smaller nudge */
}
	.getmusic-figures-row {
  display: flex;
  gap: 1.5rem;
  justify-content: center;
  align-items: flex-start;
  margin: 1.25rem 0 0.75rem;
}

.getmusic-figure {
  flex: 1 1 0;
  margin: 0;
}

.getmusic-figure img {
  width: 100%;
  height: auto;
  border-radius: 8px;
  display: block;
}
	.paper-authors {
  display: flex;
  align-items: baseline;
  gap: 0.7rem;        /* was 1.75rem */
  font-size: 0.95rem;
}

.authors-label {
  text-transform: none;       /* keep normal capitalization */
  letter-spacing: 0;          /* remove extra spacing between letters */
  font-weight: 400;           /* lighter weight */
  opacity: 0.8;               /* slightly dimmer than names */
}

.author-name {
  text-decoration: none;
}
	body {
  position: relative;
  overflow-x: hidden; /* avoid horizontal scroll from the animation */
}

html::before {
  content: "♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬";
  position: fixed;
  top: 8%;
  left: -40%;
  width: 260%;
  pointer-events: none;
  font-size: 52px;
  color: rgba(0, 0, 0, 0.22);
  white-space: nowrap;
  z-index: 0;
  animation: float-notes-1 45s linear infinite;
}
@keyframes float-notes-1 {
  0%   { transform: translateX(0); }
  100% { transform: translateX(40%); }
}
	@keyframes sweepMask {
		0% { -webkit-mask-position: 0% 0; mask-position: 0% 0; }
		100% { -webkit-mask-position: 100% 0; mask-position: 100% 0; }
	}

	/* Animated button styles */
	.btn {
		display: inline-flex;
		align-items: center;
		justify-content: center;
		padding: 10px 20px;
		border-radius: 999px;
		border: 1px solid rgba(207,207,207,0.5);
		background: radial-gradient(circle at top left, #4DA167, #3BC14A);
		color: #0D1017;
		font-size: 14px;
		font-weight: 600;
		letter-spacing: 0.06em;
		text-transform: uppercase;
		cursor: pointer;
		box-shadow:
			0 10px 25px rgba(0,0,0,0.6),
			0 0 0 1px rgba(0,0,0,0.9);
		position: relative;
		overflow: hidden;
		transition:
			transform 160ms ease,
			box-shadow 160ms ease,
			background 200ms ease,
			border-color 160ms ease;
	}

	.btn:hover {
		transform: translateY(-2px) scale(1.02);
		box-shadow:
			0 18px 40px rgba(0,0,0,0.8),
			0 0 0 1px rgba(207,207,207,0.8);
		background: radial-gradient(circle at top left, #4E6E5D, #4DA167);
		border-color: #CFCFCF;
	}

	.btn:active {
		transform: translateY(1px) scale(0.98);
		box-shadow:
			0 8px 18px rgba(0,0,0,0.9),
			0 0 0 1px rgba(207,207,207,0.6);
	}

	.btn::after {
		content: "";
		position: absolute;
		inset: 0;
		background: radial-gradient(circle at center, rgba(255,255,255,0.7) 0%, transparent 60%);
		opacity: 0;
		transform: scale(0);
		transition: opacity 260ms ease, transform 260ms ease;
		pointer-events: none;
		mix-blend-mode: screen;
	}

	.btn:active::after {
		opacity: 1;
		transform: scale(1.6);
	}
</style>

	  <title>The Platonic Representation Hypothesis</title>
      <meta property="og:title" content="The Platonic Representation Hypothesis" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align="left">
							<tr>
								<td colspan="4">
								<span class="paper-title">
									Music-Theory Informed Loss for Polyphonic MIDI Generation with Transformer Diffusion Models
								</span>
								</td>
							</tr>
							<tr>
								<td align="left" colspan="4" class="paper-authors">
  <span class="authors-label">Authors:</span>
  <a class="author-name" href="https://github.com/mihikadusad">Mihika Dusad</a>
  <a class="author-name" href="https://github.com/victoriaxhu">Victoria Hu</a>
</td>
							</tr>
							<tr>
								<td colspan="4" align="left">
								<span class="paper-subtitle">Final project for 6.7960, MIT</span>
								</td>
							</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
  <div class="outline-title">Outline</div>
  <nav class="outline-links">
    <a href="#introduction">1) Introduction</a>
    <a href="#related-work">2) Related work</a>
    <a href="#methodology">3) Methodology &amp; Experiments</a>
    <a href="#results">4) Results &amp; Analysis</a>
    <a href="#conclusions">5) Conclusions</a>
  </nav>
</div>
		    <div class="main-content-block">
			<!-- ===================== Introduction ===================== -->
<section id="introduction" class="content-section">
  <h2>Introduction</h2>
  <p class="introduction-content-section">
    AI-guided audio generation efforts have become increasingly popular, both in research and in industry [https://arxiv.org/abs/2406.00146, https://suno.com/home]. However, there is a key distinction between training models to generate pure audio and compose music. The process of composing true playable pieces -- consisting of notes, rhythms, instruments, chords -- is referred to as symbolic music generation. For symbolic music generation, we train models to work with and generate MIDI (Musical Instrument Digital Interface) files, a digital score format that encodes musical events such as pitch, duration, timing, and instrument choice. Symbolic generation often focuses on polyphonic music, where multiple notes or musical lines sound simultaneously, such as in piano pieces, string quartets, or multi-track arrangements. 
  </p>
  <p>
Most current systems represent MIDIs as a single sequence of note events, forcing the model to implicitly discover 2D structure — melodic continuity, chord consistency, and voice independence — from flattened data. Despite improvements in tokenization schemes, these models often struggle to maintain stable voice lines or valid harmonic textures. In reality, polyphonic music is inherently two-dimensional: the horizontal axis reflects how melodies evolve over time, while the vertical axis captures the harmony formed by notes sounding together at each moment. Flattening this grid into a 1D sequence obscures both dimensions, making it significantly harder for models to learn coherent melodic motion and consistent chordal structure. One framework developed by Microsoft research, GETMusic, addresses this issue by converting MIDI inputs into a 2D track–time image-like representation, enabling a diffusion-based transformer to model local and global musical patterns directly in two dimensions. This representation introduces a stronger inductive bias than single-stream event models, since vertical and horizontal relationships are spatially explicit rather than reconstructed from a token sequence. However, harmony in GETMusic remains an emergent property: the diffusion objective is a generic reconstruction loss that does not encode any explicit music-theoretic principles. As a result, GETMusic remains heavily dependent on extremely large MIDI datasets, and the original authors invested substantial effort in crawling and cleaning thousands of hours of symbolic music to obtain sufficient training data.
  </p>
  <div class="getmusic-figures-row">
    <figure class="getmusic-figure">
      <img src="./images/getmusic1_1.png" alt="GETMusic representation example 1">
    </figure>
    <figure class="getmusic-figure">
      <img src="./images/getmusic1_2.png" alt="GETMusic representation example 2">
    </figure>
  </div>
  <p>
We seek to build upon the GETMusic architecture by incorporating an explicit, music-theory-informed loss regularization term into the GETMusic framework to more reliably enforce vertical harmonic constraints. By aligning the objective function with known principles of polyphonic writing, we strengthen the inductive biases already present in GETMusic’s 2D representation and reduce the dependence on massive curated datasets. We aim to produce more coherent polyphonic outcomes, especially when training resources or datasets are limited.
  </p>
  <p>
</section>

<!-- = Related work = -->
<section id="related-work" class="content-section">
  <h2>Related work</h2>
  <h3>Sequence-based symbolic models</h3>
  <p>
    Early neural approaches to symbolic music generation modeled MIDI as a time-ordered sequence of events, typically using RNNs to generate monophonic or lightly polyphonic textures (Boulanger-Lewandowski et al., 2012)[1]. These models struggled to maintain long-range form, independent voices, and consistent harmonic context. In 2018, the transformer architecture was applied to music for the first time (Huang et al., 2018)[2], introducing relative positional self-attention for long-range structure, but encoding all notes—across hands or tracks—as a single 1D event stream. Here, vertical relationships (which notes sound together) were implied only by co-occurring time indices in the sequence, not by a structural axis. Today, the transformer is the most commonly used architecture for MIDI generation, yet virtually all widely used architectures still rely on flattened 1D event streams, leaving multi-voice structure to be reconstructed implicitly by attention.
  </p>
  <p>Some other examples of modern symbolic music generators that interpret musical input as a single stream include:</p>
  <ul>
    <li>
      <strong>Compound Word Transformer (CP-Transformer) (Hsiao et al., 2021)</strong>[3]: Groups attributes of a single note (pitch, duration, velocity, instrument) into compound tokens, shortening sequences and slightly tightening intra-note structure, but the representation remains a flat stream of notes.
    </li>
    <li>
      <strong>PopMAG / MuMIDI (Ren et al., 2020)</strong>[4]: Proposes a multi-track sequence representation where track identity and chord information are encoded as token attributes. This allows multi-track accompaniment generation but further lengthens the sequence and still interleaves tracks in one stream.
    </li>
    <li>
      <strong>MMM (Multi-Track Music Machine), MTMT (Multitrack Music Transformer), and Nested Music Transformer</strong> (Ens &amp; Pasquier, 2020; Dong et al., 2023; Ryu et al., 2024)[5–7]: Encode multi-track events as tuples (instrument, pitch, duration, etc.) that are then fully flattened into a 1D sequence.
    </li>
  </ul>
  <p>
    In these sequence-based models, harmony and voice-leading emerge from learned attention patterns over a long interleaved sequence. In practice, this creates well-known failure modes—voice crossing, unstable chord spacing, local dissonances, and loss of individual line identity—unless trained on a very large dataset and carefully regularized. Our work is motivated by this limitation: we aim to preserve the advantages of transformer-style modeling while moving towards representations and objectives that encode vertical structure more explicitly.
  </p>

  <h3>Image-based / 2D representations</h3>
  <p>
    A second family of methods abandons purely sequential representations and instead treats symbolic music as a 2D grid, usually a piano-roll with time on the horizontal axis and pitch on the vertical axis, sometimes with instrument tracks as channels. This makes vertical harmony explicit in the representation but introduces its own challenges.
  </p>
  <ul>
    <li>
      <strong>MuseGAN (Dong et al., 2017/2018)</strong>[8]: Generates multi-track piano-roll tensors, where each bar is represented as a binary matrix of size (time steps × pitches × tracks). Convolutional GANs operate directly on this 2D (plus channels) structure, so vertical chords and cross-track combinations are local patterns in an “image” rather than dispersed tokens, yielding strong local harmonic textures but typically limited to bar-level phrases with GAN training instabilities and weak global structure.
    </li>
    <li>
      <strong>MidiNet (Yang et al., 2017)</strong>[9]: Uses convolutional GANs to generate melody one bar at a time in piano-roll form, conditioned on both 1D sequences (chords) and 2D contexts (previous bars). While the vertical axis explicitly encodes chord content, the objective is still a generic adversarial loss over images rather than a music-theoretic notion of harmony.
    </li>
    <li>
      <strong>DiffRoll (Cheuk et al., 2022)</strong>[10]: Employs diffusion to generate piano-rolls conditioned on audio spectrograms for automatic music transcription. The model denoises a 2D time-pitch grid of symbolic events from Gaussian noise, demonstrating that diffusion in a piano-roll space can capture realistic local harmonic patterns.
    </li>
  </ul>
  <p>
    These 2D approaches leverage the vertical axis to make simultaneous note representations and harmony more accessible to the model, but they typically operate on high-resolution, sparse piano-roll images that are expensive to model. They also rely on distribution-matching objectives (GAN, cross-entropy/MSE, or other generic diffusion losses) that treat harmonic correctness as whatever appears frequently in the training data, without explicit constraints about voice-leading, chord function, or dissonance resolution.
  </p>

  <h3>GETMusic and 2D token grids</h3>
  <p>
    Architecturally, our work is closest to GETMusic (Lv et al., 2023)[11], which explicitly positions itself between sequence-based and image-based methods. GETMusic introduces GETScore, a 2D arrangement of discrete note tokens where tracks are stacked vertically and progress horizontally over time, with each track represented by two rows (pitch and duration). Compound pitch tokens merge simultaneous notes within a track, preserving intra-track chords while keeping the grid compact.
  </p>
  <p>
    A discrete diffusion model, GETDiff, denoises masked target tracks conditioned on source tracks in a non-autoregressive fashion, enabling flexible any-track-to-any-track generation and zero-shot infilling. Compared to piano-roll GANs/diffusion, GETScore avoids extremely large and sparse images while still making both horizontal continuity and vertical cross-track relationships spatially explicit. Compared to 1D event models such as PopMAG, it eliminates track interleaving: tracks are temporally aligned in a score-like grid rather than interwoven in a single stream.
  </p>
  <p>
    However, GETMusic’s objective remains a purely statistical discrete diffusion loss plus auxiliary reconstruction terms. There is no explicit bias towards music-theoretic principles beyond what is implicitly encoded in the dataset and the compound-token construction. In fact, the authors emphasize that they crawled and cleaned over 1.5M MuseScore MIDIs (around 2,800 hours of data) to train GETMusic, underscoring its dependence on very large curated datasets to learn harmonic structure robustly.
  </p>

  <h3>Positioning our contribution</h3>
  <p>
    Our project builds directly on this GETMusic line of work: we retain a 2D symbolic representation and a diffusion-style transformer backbone, but modify the learning objective by adding a music-theory-informed regularization term that explicitly evaluates the vertical slice (local harmony and spacing at each time step). Rather than treating harmony as a purely emergent property of data and generic denoising, we encode constraints derived from polyphonic writing practice—for example, penalties for certain dissonant intervals or unstable chord spacings, and rewards for consonant interval structures and consistent voice ranges—and integrate them into the loss function calculation.
  </p>
</section>

<!-- = Non-transformer models with explicit 2D inductive biases = -->
<section id="non-transformer-2d" class="content-section">
  <h2>Non-transformer models with explicit 2D inductive biases</h2>
  <p>
    Prior to the Transformer era, several models demonstrated that explicitly modeling the 2D music grid (voices × time) produces more coherent voice-leading and harmony:
  </p>
  <ul>
    <li>
      <strong>DeepBach (Hadjeres et al., 2017)</strong>: Models chorales as a matrix of pitches over discrete time steps; the sampling process treats vertical slices of the score as first-class objects.
    </li>
    <li>
      <strong>Coconet / Counterpoint by Convolution (Huang et al., 2019)</strong>: Uses a large convolutional network over a 2D piano-roll representation (pitch × time) to enable inpainting and harmony generation.
    </li>
  </ul>
  <p>
    These systems succeed largely because they directly encode vertical relationships, making correct chord structures and spacing easier for the network to learn. However, they are largely restricted to four-part choral textures and do not readily generalize to multi-track MIDI with arbitrary instrumentation, expressive timing, or long pieces. Moreover, they lack the generative flexibility and scalability of modern diffusion or transformer models.
  </p>
</section>

<!-- = Choosing GETMusic as a baseline: MIDI score to image representation = -->
<section id="getmusic-baseline" class="content-section">
  <h2>Choosing GETMusic as a baseline: MIDI score to image representation</h2>
  <p>
    GETMusic combines a 2D symbolic representation with diffusion-based generation, bridging the gap between classical 2D harmony models and modern deep generative architectures. Rather than representing MIDI as a sequence of events, GETMusic rasterizes symbolic music into a pitch × time grid representation known as GETScore. This creates a multi-channel “image” where horizontal continuity corresponds to melodic flow and vertical stacks correspond to chords or polyphonic textures.
  </p>
  <p>
    GETMusic applies a discrete diffusion model, GETDiff, to the GETScore representation to progressively reconstruct clean symbolic tracks from masked, corrupted inputs. The denoiser is implemented as a transformer with RoFormer layers: the model embeds the 2D GETScore grid (tracks arranged vertically, time units horizontally, with separate rows for pitch and duration per track), adds learnable condition flags that indicate which tokens are sources/targets/ignored, projects into a model dimension, and then feeds the sequence through 12 RoFormer layers followed by a classification head over the token vocabulary.
  </p>
  <p>
    In the forward (diffusion) process, target-track tokens are iteratively corrupted by transitioning them toward a special mask token according to a categorical Markov chain, while source-track tokens are kept as ground truth and uninvolved tracks are filled with a special empty token that does not diffuse. In the denoising process, at each timestep GETDiff predicts the clean token distribution for all positions non-autoregressively, i.e., it generates all tokens in parallel conditioned on the current noisy GETScore, the source tracks, and condition flags; after each reverse step, source tracks are reset to their ground truth values and uninvolved tracks are re-emptied.
  </p>
  <p>
    By randomly sampling source and target tracks during training, the same model can handle any source-target combination without retraining. In this case, there are tracks for lead melody, piano, guitar, strings, bass, and drums. GETScore’s vertically stacked, temporally aligned tracks preserve interdependencies among simultaneous notes within and across tracks, so GETDiff always denoises in a space where track separations and time alignment are explicit.
  </p>
</section>

<!-- = Architectural limitations = -->
<section id="architectural-limitations" class="content-section">
  <h2>Architectural limitations</h2>
  <p>
    GETScore and GETDiff together introduce a stronger structural bias than standard sequence-based models: tracks are explicitly separated yet temporally aligned, and within-track simultaneity is compacted into compound pitch tokens derived from real data. However, from an optimization perspective, GETMusic is still trained with a generic discrete diffusion objective over tokens: the loss is a variational lower bound with categorical transitions and an additional x₀-parameterization term, effectively encouraging the model to reconstruct the original GETScore tokens from noisy versions.
  </p>
  <p>
    Crucially, this objective does not encode any explicit music-theoretic constraints. In particular, GETMusic does not directly model or penalize chord plausibility or spacing (beyond what is implicit in the data), or stylistic consistency of harmony or voice leading. Harmony and voice leading are therefore emergent properties that the model must infer implicitly from the training corpus, rather than being enforced by the loss. This interacts with the inherent data demands of diffusion: GETDiff is trained with 100 denoising steps over a large vocabulary (11,883 tokens) and a 2D multi-track grid, so learning robust harmonic behavior requires substantial coverage of stylistic and structural patterns. In practice, the authors mitigate this by crawling 1.56M MIDI files from MuseScore and constructing 137,812 GETScores (approximately 2,800 hours worth) after strict cleaning and preprocessing, but assembling such a dataset and pipeline is nontrivial for the average person.
  </p>
  <p>
    These factors motivate our approach: rather than relying solely on large-scale data and a generic categorical diffusion objective to “discover” harmonic structure, we introduce music-theory-informed regularization terms that explicitly favor vertically coherent harmony within the GETMusic framework.
  </p>
</section>

<!-- = Adding explicit music-theory-informed loss regularization term = -->
<section id="loss-regularization" class="content-section">
  <h2>Adding explicit music-theory-informed loss regularization term</h2>
  <p><em>(Content to be added.)</em></p>
</section>

<!-- = Methodology & Experiments = -->
<section id="methodology" class="content-section">
  <h2>Methodology &amp; Experiments</h2>
  <p><em>(Content to be added.)</em></p>
</section>

<!-- = Results & Analysis = -->
<section id="results" class="content-section">
  <h2>Results &amp; Analysis</h2>
  <p><em>(Content to be added.)</em></p>
</section>

<!-- = Conclusions = -->
<section id="conclusions" class="content-section">
  <h2>Conclusions</h2>
  <p><em>(Content to be added.)</em></p>
</section>
			<div class="project-description">
  <div class="project-description">
</div>
</div>
		</div>
		    <div class="margin-right-block">
					
		    </div>
		</div>

		<div class="content-margin-container" id="does_x_do_y">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Does X do Y?</h1>
				  It is well known that Y does Y. And this has raised the question does X do Y? Because if Y does Y then it stands to reason that X does Y.
          But we cannot answer this until we realize the Z implies Y and X can be linked to Z.<br><br>

          Now let's write some math!<br>
          <center>
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <mi>y</mi>
                </mrow>
                <mo>/</mo>
                <mrow>
                  <mo>&#x2202;</mo>
                  <mi>x</mi>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mi>x</mi>
            </math>
          </center>
          <br>
          It's probably best to ask an LLM to help do the web
          formatting for math. You can tell it "convert this latex equation into MathML: $$\frac{\partial dy}{\partial dx} = x$$"
          But it took me a few tries. So, if you get frustrated, you can embed an image of the equation, or use other packages for
          rendering equations on webpages.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Another section</h1>
            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		    <div class="margin-right-block">
					A caption for the video could go here.
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and limitations</h1>
						Let's end with some discussion of the implications and limitations.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:24px">References:</span><br><br>
							[1] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent.
							<em>Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription.</em>
							In <em>Proceedings of the 29th International Conference on Machine Learning (ICML)</em>, 2012.
							arXiv:1206.6392.<br><br>

							[2] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, et al.
							<em>Music Transformer: Generating Music with Long-Term Structure.</em>
							In <em>International Conference on Learning Representations (ICLR)</em>, 2019.
							arXiv:1809.04281.<br><br>

							[3] W.-Y. Hsiao, J.-Y. Liu, Y.-H. Yang, and I. Liao.
							<em>Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs.</em>
							In <em>AAAI Conference on Artificial Intelligence</em>, 2021.
							arXiv:2101.02402.<br><br>

							[4] Y. Ren, J. He, X. Tan, et al.
							<em>PopMAG: Pop Music Accompaniment Generation.</em>
							In <em>Proceedings of the 28th ACM International Conference on Multimedia</em>, 2020.
							arXiv:2008.07703.<br><br>

							[5] J. Ens and P. Pasquier.
							<em>MMM: Exploring Conditional Multi-Track Music Generation with the Transformer.</em>
							In <em>Proceedings of the 21st International Society for Music Information Retrieval Conference (ISMIR)</em>, 2020.
							arXiv:2008.06048.<br><br>

							[6] H.-W. Dong, K. Chen, S. Dubnov, J. McAuley, and T. Berg-Kirkpatrick.
							<em>Multitrack Music Transformer: Learning Long-Term Dependencies in Music with Diverse Instruments.</em>
							In <em>ICASSP</em>, 2023.
							arXiv:2207.06983.<br><br>

							[7] K. Ryu, S. Cho, H. Choi, et al.
							<em>Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music.</em>
							In <em>Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)</em>, 2024.
							arXiv:2408.01180.<br><br>

							[8] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang.
							<em>MuseGAN: Multi-Track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment.</em>
							In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2018.
							arXiv:1709.06298.<br><br>

							[9] L.-C. Yang, S.-Y. Chou, and Y.-H. Yang.
							<em>MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation.</em>
							In <em>Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)</em>, 2017.
							arXiv:1703.10847.<br><br>

							[10] K. W. Cheuk, R. Sawata, T. Uesaka, et al.
							<em>DiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability.</em>
							In <em>ICASSP</em>, 2023.
							arXiv:2210.05148.<br><br>

							[11] Z. Lv, Z. Dai, G. Li, et al.
							<em>GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework.</em>
							arXiv:2305.10841, 2023.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	<script>
  document.querySelectorAll('.content-card').forEach(function(card) {
    card.addEventListener('mousemove', function(e) {
      const rect = card.getBoundingClientRect();
      const x = e.clientX - rect.left;
      const y = e.clientY - rect.top;
      card.style.setProperty('--glow-x', x + 'px');
      card.style.setProperty('--glow-y', y + 'px');
    });

    card.addEventListener('mouseleave', function() {
      // Optional: fade glow back to center when leaving
      card.style.removeProperty('--glow-x');
      card.style.removeProperty('--glow-y');
    });
  });
</script>
	</body>

</html>
