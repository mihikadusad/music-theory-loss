# -*- coding: utf-8 -*-
"""DL_Music_Gen_Baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aIZ2nT9mgTYD26HISR2FzAYLYyT54BMN
"""

# Commented out IPython magic to ensure Python compatibility.
# Run this in a FRESH Colab runtime.

!rm -rf /content/muzic
# %cd /content

# Clone Muzic repo
!git clone https://github.com/microsoft/muzic.git
# %cd /content/muzic/getmusic
!ls

# Install PyTorch + libs compatible with GETMusic
#!pip install -q "torch==1.12.1" "torchvision==0.13.1" "torchaudio==0.12.1"
!pip install torch torchvision torchaudio

!pip install -q tensorboard pyyaml tqdm transformers einops scipy
!pip install miditoolkit==0.1.16

# Commented out IPython magic to ensure Python compatibility.
#PATCH TO THE LR SCHEDULER CODE
# %cd /content/muzic/getmusic
import pathlib

lr_path = pathlib.Path("getmusic/engine/lr_scheduler.py")
text = lr_path.read_text()

if "from torch._six import inf" in text:
    text = text.replace(
        "from torch._six import inf",
        "from math import inf  # patched: torch._six removed in newer Torch"
    )
    lr_path.write_text(text)
    print("‚úÖ Patched getmusic/engine/lr_scheduler.py (torch._six ‚Üí math.inf)")
else:
    print("‚ÑπÔ∏è lr_scheduler.py already patched or doesn't use torch._six.")

# Commented out IPython magic to ensure Python compatibility.
# #PATCH TO NUMPY VERSION INCOMPABILITY BUGS
# 
# %%bash
# FILE="preprocess/to_oct.py"
# 
# # Insert NumPy shim right after the first "import numpy as np"
# # but BEFORE miditoolkit import.
# 
# # Create a backup
# cp $FILE ${FILE}.bak
# 
# # Use awk to inject the shim after the numpy import
# awk '
# /import numpy as np/ {
#     print $0;
#     print "";
#     print "# === NumPy 2.x compatibility shim ===";
#     print "import numpy as _np";
#     print "if not hasattr(_np, \"int\"): _np.int = int";
#     print "if not hasattr(_np, \"float\"): _np.float = float";
#     print "if not hasattr(_np, \"bool\"): _np.bool = bool";
#     print "# =====================================";
#     next;
# }
# { print }
# ' ${FILE}.bak > $FILE
# 
# echo "Patch applied to $FILE"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import os

PROCESSED_DIR = "example_data/processed_train"
print("Using PROCESSED_DIR:", PROCESSED_DIR)

# Make sure the output dir does NOT exist before to_oct.py
!rm -rf {PROCESSED_DIR}

print("Train MIDIs that will be processed:")
!ls example_data/train

print("\nRunning preprocess/to_oct.py ...\n")
#!python preprocess/to_oct.py example_data/train {PROCESSED_DIR} | tee to_oct_log.txt
!python preprocess/to_oct.py example_data/train {PROCESSED_DIR} 2>&1 | tee to_oct_log.txt

print("\nContents of", PROCESSED_DIR, ":")
!ls -la {PROCESSED_DIR}

oct_path = os.path.join(PROCESSED_DIR, "oct.txt")
if not os.path.exists(oct_path):
    print("\n‚ùå oct.txt was NOT created. Last 40 lines of to_oct_log.txt:")
    !tail -n 40 to_oct_log.txt
else:
    print("\n‚úÖ oct.txt exists at", oct_path)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import ast, re, pathlib

print("Using PROCESSED_DIR:", PROCESSED_DIR)

# Build dict
!python preprocess/make_dict.py {PROCESSED_DIR}/ 3 | tee make_dict_log.txt

log_lines = open("make_dict_log.txt", "r").read().splitlines()
list_lines = [ln for ln in log_lines if ln.strip().startswith('[') and ln.strip().endswith(']')]

if len(list_lines) < 2:
    raise RuntimeError("Could not find tracks_start/tracks_end in make_dict output.")

start_ls = ast.literal_eval(list_lines[-2])
end_ls   = ast.literal_eval(list_lines[-1])

print("tracks_start:", start_ls)
print("tracks_end  :", end_ls)

# Patch midi_config.py
mc_path = pathlib.Path("getmusic/utils/midi_config.py")
text = mc_path.read_text()

text = re.sub(r"tracks_start\s*=\s*\[.*?\]", f"tracks_start = {start_ls}", text)
text = re.sub(r"tracks_end\s*=\s*\[.*?\]", f"tracks_end = {end_ls}", text)

mc_path.write_text(text)
print("\n‚úÖ Updated getmusic/utils/midi_config.py with new tracks_start/tracks_end.")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import os

print("Using PROCESSED_DIR:", PROCESSED_DIR)

pitch_dict_path = os.path.join(PROCESSED_DIR, "pitch_dict.txt")
if not os.path.exists(pitch_dict_path):
    raise FileNotFoundError(f"{pitch_dict_path} not found; make_dict may have failed.")

!python preprocess/binarize.py \
  {pitch_dict_path} \
  {PROCESSED_DIR}/oct.txt \
  {PROCESSED_DIR}

print("\nContents of", PROCESSED_DIR, "after binarize:")
!ls -la {PROCESSED_DIR}

# Commented out IPython magic to ensure Python compatibility.
#PATCH based on getmusic instructions, should be run
# %cd /content/muzic/getmusic
import pathlib, re

TRAIN_YAML = pathlib.Path("configs/train.yaml")
text = TRAIN_YAML.read_text()

# Compute vocab_size = (#lines in pitch_dict.txt) + 1
pitch_dict_path = pathlib.Path(PROCESSED_DIR) / "pitch_dict.txt"
num_tokens = sum(1 for _ in pitch_dict_path.open("r", encoding="utf-8") if _.strip())
vocab_size = num_tokens + 1

print("Tokens in pitch_dict.txt:", num_tokens)
print("=> vocab_size we will set:", vocab_size)

# 1) Update ALL vocab_size occurrences
text = re.sub(r"vocab_size:\s*\d+", f"vocab_size: {vocab_size}", text)

# 2) Update vocab_path
# (only first match should be the solver's vocab_path)
text = re.sub(r"vocab_path:\s*\S+", f"vocab_path: {PROCESSED_DIR}/pitch_dict.txt", text, count=1)

# 3) Update all data_folder occurrences (train + valid)
text = re.sub(r"data_folder:\s*\S+", f"data_folder: {PROCESSED_DIR}", text)

# 4) Also replace any '/your-data-path' leftovers, just in case
text = text.replace("/your-data-path", PROCESSED_DIR)

TRAIN_YAML.write_text(text)
print("\n‚úÖ Patched configs/train.yaml (vocab_size, vocab_path, data_folder).")

print("\nSanity check for /your-data-path:")
!grep -R "your-data-path" -n configs || echo "No /your-data-path remains in configs."

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import pathlib, re

tp = pathlib.Path("train.py")
text = tp.read_text()

# We look for the line `solver.train()` in main_worker and append a manual save right after it.
pattern = "    solver = Solver(config=config, args=args, model=model, dataloader=dataloader_info, logger=logger)\n" \
          "    solver.train()\n"

replacement = (
    "    solver = Solver(config=config, args=args, model=model, dataloader=dataloader_info, logger=logger)\n"
    "    solver.train()\n"
    "    # --- manual checkpoint save added for Colab ---\n"
    "    import torch, os\n"
    "    os.makedirs('OUTPUT', exist_ok=True)\n"
    "    torch.save(model.state_dict(), 'OUTPUT/manual_final.pth')\n"
)

if pattern not in text:
    # Fallback: only replace the `solver.train()` line, keeping indentation
    text = text.replace(
        "    solver.train()\n",
        "    solver.train()\n"
        "    # --- manual checkpoint save added for Colab (fallback) ---\n"
        "    import torch, os\n"
        "    os.makedirs('OUTPUT', exist_ok=True)\n"
        "    torch.save(model.state_dict(), 'OUTPUT/manual_final.pth')\n"
    )
    print("‚ö†Ô∏è Pattern not matched exactly; used fallback replacement.")
else:
    text = text.replace(pattern, replacement)
    print("‚úÖ Patched train.py to save OUTPUT/manual_final.pth after training.")

tp.write_text(text)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import pathlib, re

TRAIN_YAML = pathlib.Path("configs/train.yaml")
text = TRAIN_YAML.read_text()

# Add checkpoint-related keys under 'solver:' if not already present
if "save_checkpoints:" not in text:
    # Insert right after the 'solver:' line
    text = re.sub(
        r'^solver:\s*$',           # a line that is exactly "solver:"
        'solver:\n'
        '  save_checkpoints: True\n'
        '  checkpoint_dir: OUTPUT/checkpoints\n'
        '  save_epochs: 1',
        text,
        count=1,
        flags=re.MULTILINE,
    )
    TRAIN_YAML.write_text(text)
    print("‚úÖ Added save_checkpoints / checkpoint_dir / save_epochs under solver.")
else:
    print("‚ÑπÔ∏è Checkpoint fields already present in configs/train.yaml.")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
!python train.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic

# Make an inference folder if you don't already have one
import os, shutil
os.makedirs("example_data/inference", exist_ok=True)

# For sanity, copy one demo MIDI into inference:
demo_src = "example_data/train/0_01023_TS0.mid"
shutil.copy(demo_src, "example_data/inference/0_01023_TS0.mid")

print("Inference MIDIs:")
!ls example_data/inference

# Run GETMusic track generation using the trained weights
#lol jk there's no trained weights yet
#!python track_generation.py \
 # --load_path OUTPUT/manual_final.pth \
 # --file_path example_data/inference

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import torch, os

raw_path = "OUTPUT/manual_final.pth"
if not os.path.exists(raw_path):
    raise FileNotFoundError(f"{raw_path} not found. Did train.py finish and save it?")

# This is the pure model.state_dict() we saved in train.py
state_dict = torch.load(raw_path, map_location="cpu")

# Wrap in the format Solver.resume() expects
wrapped = {
    "last_epoch": 0,
    "global_step": 0,
    "state_dict": state_dict,
    # These won't be used because track_generation sets no_load_optimizer_and_scheduler=True,
    # but we include them for completeness.
    "optimizer": {},
    "scheduler": {},
}

wrapped_path = "OUTPUT/manual_solver.pth"
torch.save(wrapped, wrapped_path)
print("‚úÖ Wrote wrapped checkpoint:", wrapped_path)

from google.colab import drive
drive.mount('/content/drive')

import os
import shutil

# Define the source directory where your .pth files are located
source_dir = "/content/muzic/getmusic/OUTPUT"

# Define the destination folder in your Google Drive
# You can change 'GETMusic_Checkpoints' to any folder name you prefer.
# The path starts from '/content/drive/MyDrive/' after mounting.
destination_folder = "/content/drive/MyDrive/Colab Notebooks/GETMusic_Checkpoints"

# Create the destination folder in Google Drive if it doesn't exist
os.makedirs(destination_folder, exist_ok=True)

# Paths to the files you want to copy
file_to_copy1 = os.path.join(source_dir, "manual_final.pth")
file_to_copy2 = os.path.join(source_dir, "manual_solver.pth")

# Copy manual_final.pth
if os.path.exists(file_to_copy1):
    shutil.copy(file_to_copy1, destination_folder)
    print(f"‚úÖ Copied {os.path.basename(file_to_copy1)} to {destination_folder}")
else:
    print(f"‚ùå {os.path.basename(file_to_copy1)} not found in {source_dir}")

# Copy manual_solver.pth
if os.path.exists(file_to_copy2):
    shutil.copy(file_to_copy2, destination_folder)
    print(f"‚úÖ Copied {os.path.basename(file_to_copy2)} to {destination_folder}")
else:
    print(f"‚ùå {os.path.basename(file_to_copy2)} not found in {source_dir}")

print("\nDone copying checkpoints to Google Drive.")

"""Inference"""

from google.colab import drive
drive.mount('/content/drive')

import os
import shutil

#retrieve checkpoints from google drive
#source_folder_drive = "/content/drive/MyDrive/Colab Notebooks/GETMusic_Checkpoints"
source_folder_drive = "/content/drive/MyDrive/datasets/getmusic_OUTPUT"

# Define the destination directory in the Colab environment
destination_folder_colab = "/content/muzic/getmusic/OUTPUT"

# Create the destination folder in Colab if it doesn't exist
os.makedirs(destination_folder_colab, exist_ok=True)

# File names
file1_name = "manual_final.pth"
file2_name = "manual_solver.pth"

# Full source and destination paths for manual_final.pth
source_path1 = os.path.join(source_folder_drive, file1_name)
dest_path1 = os.path.join(destination_folder_colab, file1_name)

# Full source and destination paths for manual_solver.pth
source_path2 = os.path.join(source_folder_drive, file2_name)
dest_path2 = os.path.join(destination_folder_colab, file2_name)

print(f"Retrieving files from: {source_folder_drive}")

# Copy manual_final.pth
if os.path.exists(source_path1):
    shutil.copy(source_path1, dest_path1)
    print(f"‚úÖ Copied {file1_name} to {destination_folder_colab}")
else:
    print(f"‚ùå {file1_name} not found in {source_folder_drive}")

# Copy manual_solver.pth
if os.path.exists(source_path2):
    shutil.copy(source_path2, dest_path2)
    print(f"‚úÖ Copied {file2_name} to {destination_folder_colab}")
else:
    print(f"‚ùå {file2_name} not found in {source_folder_drive}")

print("\nDone retrieving checkpoints.")

#add a block of code which retrieves the checkpoints saved every 2 epochs
import os
import shutil

# Source: specific training run's checkpoint dir on Drive
source_folder_drive = "/content/drive/MyDrive/datasets/getmusic_OUTPUT/train/2025-12-08T09-54-07/checkpoint/"

# Destination: checkpoint dir inside the local repo
destination_folder_colab = "/content/muzic/getmusic/OUTPUT/checkpoint"

# Make sure destination exists
os.makedirs(destination_folder_colab, exist_ok=True)

print(f"Retrieving checkpoints from: {source_folder_drive}")
if not os.path.exists(source_folder_drive):
    raise FileNotFoundError(f"‚ùå Source folder not found: {source_folder_drive}")

# List all files in source and filter for .pth
all_files = os.listdir(source_folder_drive)
pth_files = [f for f in all_files if f.endswith(".pth")]

if not pth_files:
    print("‚ö†Ô∏è No .pth checkpoint files found in source folder.")
else:
    for fname in pth_files:
        src = os.path.join(source_folder_drive, fname)
        dst = os.path.join(destination_folder_colab, fname)
        shutil.copy(src, dst)
        print(f"‚úÖ Copied {fname} -> {destination_folder_colab}")

print("\nDone retrieving checkpoints.")
print("Local checkpoint directory contents:")
print(os.listdir(destination_folder_colab))

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import os

print("Files in OUTPUT:")
!ls -la OUTPUT

assert os.path.exists("OUTPUT/manual_final.pth"), "manual_final.pth not found in OUTPUT"
print("‚úÖ manual_final.pth exists")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
from pathlib import Path

solver_path = Path("getmusic/engine/solver.py")
text = solver_path.read_text()

patch = """

# --------------------------------------------------------
# Patched resume() to support both full checkpoints and
# plain model.state_dict() files (e.g., manual_final.pth)
# --------------------------------------------------------
def _patched_resume(self, path):
    import torch

    ckpt = torch.load(path, map_location='cpu')

    # Case 1: training-style checkpoint with 'state_dict'
    if isinstance(ckpt, dict) and 'state_dict' in ckpt:
        self.last_epoch = ckpt.get('last_epoch', 0)
        self.global_step = ckpt.get('global_step', 0)

        self.model.load_state_dict(ckpt['state_dict'])

        # Only load optimizer/scheduler if allowed
        if not getattr(self.args, "no_load_optimizer_and_scheduler", False):
            if 'optimizer' in ckpt and hasattr(self, "optimizer"):
                try:
                    self.optimizer.load_state_dict(ckpt['optimizer'])
                except Exception:
                    pass
            if 'scheduler' in ckpt and hasattr(self, "scheduler"):
                try:
                    self.scheduler.load_state_dict(ckpt['scheduler'])
                except Exception:
                    pass

    # Case 2: plain state_dict (manual_final.pth)
    else:
        self.last_epoch = 0
        self.global_step = 0
        loaded = False

        # try direct load
        try:
            self.model.load_state_dict(ckpt)
            loaded = True
        except Exception:
            pass

        # try nested dicts (rare)
        if not loaded and isinstance(ckpt, dict):
            for v in ckpt.values():
                try:
                    self.model.load_state_dict(v)
                    loaded = True
                    break
                except Exception:
                    continue

    print(f"Resumed checkpoint from {path}, last_epoch={self.last_epoch}, global_step={self.global_step}")

# Replace original resume with patched version
Solver.resume = _patched_resume
"""

# Append patch safely
if "_patched_resume" not in text:
    solver_path.write_text(text + "\n" + patch)
    print("‚úÖ Appended clean patched resume() to solver.py")
else:
    print("‚ÑπÔ∏è Patched resume already present.")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic
!git restore getmusic/track_generation.py

#PATCH FOR NP.INT BEING DEPRECATED
import pathlib
import re

# Path to the miditoolkit parser.py file
parser_path = pathlib.Path("/usr/local/lib/python3.12/dist-packages/miditoolkit/midi/parser.py")

# Read the file content
text = parser_path.read_text()

# Replace 'np.int' with 'int' at the problematic line
# The traceback indicated line 205: current_instrument = np.zeros(16, dtype=np.int)
# We'll target this specific pattern for replacement
old_code = "np.zeros(16, dtype=np.int)"
new_code = "np.zeros(16, dtype=int)"

if old_code in text:
    text = text.replace(old_code, new_code)
    parser_path.write_text(text)
    print("‚úÖ Patched miditoolkit/midi/parser.py: Replaced 'np.int' with 'int'.")
else:
    print("‚ÑπÔ∏è miditoolkit/midi/parser.py already patched or code changed.")

#PATCH FOR OUT OF RANGE CLAMPING + SAFE OUTPUTS DIR
from pathlib import Path
import re
import os

# Adjust this path if your repo is elsewhere
TG_PATH = Path("/content/muzic/getmusic/track_generation.py")
assert TG_PATH.exists(), f"track_generation.py not found at {TG_PATH}"

text = TG_PATH.read_text()

print("‚úÖ Loaded", TG_PATH)

# ---------------------------------------------------------------------
# 1. Inject helper function after `import miditoolkit` if not present
# ---------------------------------------------------------------------
helper_code = r"""
# --- START PATCH: MIDI data byte clamping helper (added in Colab) ---
def _clamp_midi_byte(value, name="value", min_val=0, max_val=127):

    #Clamp any MIDI data-like value into [min_val, max_val], logging if adjusted.

    try:
        original = value
        value = int(round(float(value)))
    except Exception:
        print(f"[MIDI CLAMP] {name}: non-numeric '{value}', using {min_val}")
        return min_val

    if value < min_val or value > max_val:
        clamped = max(min_val, min(max_val, value))
        print(f"[MIDI CLAMP] {name}: {original} -> {clamped} (range {min_val}..{max_val})")
        return clamped
    return value
# --- END PATCH ---
"""

if "_clamp_midi_byte(" not in text:
    marker = "import miditoolkit"
    if marker not in text:
        raise RuntimeError("Could not find 'import miditoolkit' in track_generation.py")
    text = text.replace(marker, marker + helper_code)
    print("‚úÖ Injected _clamp_midi_byte helper.")
else:
    print("‚ÑπÔ∏è Helper already present, skipping injection.")

# ---------------------------------------------------------------------
# 2. Patch chord Note creation (program == 129)
# ---------------------------------------------------------------------
old_chord_block = """            midi_obj.instruments[1].notes.append(miditoolkit.containers.Note(
                start=start, end=end, pitch=pitch, velocity=e2v(20)))"""

new_chord_block = """            pitch = _clamp_midi_byte(pitch, name="chord pitch", min_val=0, max_val=127)
            velocity = _clamp_midi_byte(e2v(20), name="chord velocity", min_val=1, max_val=127)
            midi_obj.instruments[1].notes.append(miditoolkit.containers.Note(
                start=start, end=end, pitch=pitch, velocity=velocity))"""

if old_chord_block in text and "chord pitch" not in text:
    text = text.replace(old_chord_block, new_chord_block)
    print("‚úÖ Patched chord Note creation with clamping.")
else:
    print("‚ÑπÔ∏è Chord Note block already patched or not found; skipping.")

# ---------------------------------------------------------------------
# 3. Patch normal Note creation (program != 129)
# ---------------------------------------------------------------------
old_note_block = """            midi_obj.instruments[program].notes.append(miditoolkit.containers.Note(
                start=start, end=end, pitch=pitch, velocity=velocity))"""

new_note_block = """            pitch = _clamp_midi_byte(pitch, name="note pitch", min_val=0, max_val=127)
            velocity = _clamp_midi_byte(velocity, name="note velocity", min_val=1, max_val=127)
            midi_obj.instruments[program].notes.append(miditoolkit.containers.Note(
                start=start, end=end, pitch=pitch, velocity=velocity))"""

if old_note_block in text and "note pitch" not in text:
    text = text.replace(old_note_block, new_note_block)
    print("‚úÖ Patched normal Note creation with clamping.")
else:
    print("‚ÑπÔ∏è Normal Note block already patched or not found; skipping.")

# ---------------------------------------------------------------------
# 4. Patch save_path to force output into `outputs/`
# ---------------------------------------------------------------------
save_pattern = r"""save_path\s*=\s*os\.path\.join\(args\.file_path,\s*['"]\{\}2\{\}-\{'?"""
# But simpler: look for the entire line

save_line_pattern = r"save_path = os\.path\.join\(args\.file_path, (.+)\)"
match = re.search(save_line_pattern, text)

if match:
    original_line = match.group(0)

    new_line = (
        "os.makedirs(os.path.join(args.file_path, 'outputs'), exist_ok=True)\n"
        "        save_path = os.path.join(args.file_path, 'outputs', {})"
    ).format(match.group(1))

    text = text.replace(original_line, new_line)
    print("‚úÖ Patched save_path to use args.file_path/outputs/")
else:
    print("‚ö†Ô∏è Could not find save_path line to patch. Manual fix needed!")

# ---------------------------------------------------------------------
# 5. Write back
# ---------------------------------------------------------------------
TG_PATH.write_text(text)
print("‚ú® Done patching track_generation.py with clamping + safe output directory!")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
!ls "example_data/inference"

!python track_generation.py \
  --load_path OUTPUT/manual_solver.pth \
  --file_path example_data/inference

"""Code to play MIDI files as audio in colab
can play sample input as well as output of conditional generation
"""

!apt-get install -y fluidsynth
!pip install midi2audio

!wget -q https://github.com/blocksized/GM-Soundfont/raw/master/SGM-V2.01.sf2

from midi2audio import FluidSynth
from IPython.display import Audio

def createAudio(midi_path, wav_path):
  fs = FluidSynth("SGM-V2.01.sf2")
  fs.midi_to_audio(midi_path, wav_path)

  return Audio(wav_path, rate=44100)

!ls "example_data/inference/outputs"

createAudio("example_data/inference/outputs/l2g-childhood.mid", "example_data/inference/testing1.wav")

createAudio("example_data/inference/outputs/l2p-0_01023_TS0.mid", "example_data/inference/testing1output.wav")

"""Now we use Slakh2100 to train properly fr fr this time
- download straight to drive so that persist across sessions
- choose good pieces with multiple tracks like lead, piano, guitar, drums, bass, chords (optional)
- update training parameters for this particular run, track using wandb

"""

from google.colab import drive
drive.mount('/content/drive')

# ============================================================
# STEP 1 ‚Äî Download Slakh2100 (slakh2100_flac_redux.tar.gz)
#         DIRECTLY INTO GOOGLE DRIVE (persistent)
# ============================================================
import os, tarfile, pathlib

# Path to your Google Drive dataset location
GDRIVE_BASE = "/content/drive/MyDrive/datasets"
SLAKH_DIR = os.path.join(GDRIVE_BASE, "slakh2100")
ARCHIVE_NAME = "slakh2100_flac_redux.tar.gz"

os.makedirs(SLAKH_DIR, exist_ok=True)
print("üìÅ Dataset base directory:", SLAKH_DIR)

ARCHIVE_PATH = os.path.join(SLAKH_DIR, ARCHIVE_NAME)

# ------------------------------------------------------------
# 1) Download from Zenodo (correct URL including ?download=1)
# ------------------------------------------------------------
print("\n‚¨áÔ∏è Downloading Slakh2100 from Zenodo...")
!wget -O "{ARCHIVE_PATH}" \
   "https://zenodo.org/records/4599666/files/slakh2100_flac_redux.tar.gz?download=1"

print("\nDownloaded OK:", os.path.exists(ARCHIVE_PATH), "->", ARCHIVE_PATH)

# ------------------------------------------------------------
# 2) Extract the TAR.GZ archive to a persistent folder in Drive
# ------------------------------------------------------------
EXTRACT_DIR = os.path.join(SLAKH_DIR, "extracted")
os.makedirs(EXTRACT_DIR, exist_ok=True)

print("\nüì¶ Extracting into:", EXTRACT_DIR)

# Use tarfile with mode auto-detection "r:*" (supports gzip, tar, etc.)
try:
    with tarfile.open(ARCHIVE_PATH, "r:*") as tar:
        tar.extractall(path=EXTRACT_DIR)
    print("‚úÖ Extraction successful!")
except Exception as e:
    print("‚ùå Extraction failed:", e)

print("\nüìÅ Extracted contents:")
!ls -la "{EXTRACT_DIR}"

"""We let it download asynchronously bc super huge dataset, and now we retrieve it"""

from google.colab import drive
drive.mount("/content/drive")

import os

SLAKH_ROOT = "/content/drive/MyDrive/datasets/slakh2100/extracted/slakh2100_flac_redux"
print("Slakh root:", SLAKH_ROOT)

import os
from pathlib import Path

root = Path(SLAKH_ROOT)

subfolders = ["train", "validation", "test", "omitted"]

for sub in subfolders:
    folder = root / sub
    if not folder.exists():
        print(f"{sub}: ‚ùå does not exist")
        continue

    # Count only directories (songs)
    songs = [p for p in folder.iterdir() if p.is_dir()]

    # Count any other files (rare)
    files = [p for p in folder.iterdir() if p.is_file()]

    print(f"{sub}:")
    print(f"  Song directories: {len(songs)}")
    print(f"  Other files:      {len(files)}")
    print()

import os
track = "/content/drive/MyDrive/datasets/slakh2100/extracted/slakh2100_flac_redux/train/Track00302"
print("Files in Track00302:", os.listdir(track))
print("Files in Track00302/MIDI:", os.listdir(track + "/MIDI"))

!pip install pretty_midi pyyaml tqdm

from pathlib import Path
import pretty_midi
import yaml
from tqdm import tqdm

SRC = Path(SLAKH_ROOT)
DEST = Path("/content/slakh2100-merged-tracks")
DEST.mkdir(parents=True, exist_ok=True)

print("Source Slakh dir:", SRC)
print("Destination (7-track) root:", DEST)

splits = ["train", "validation", "test", "omitted"]

# --- Instrument family -> 7 canonical tracks mapping ---
# 0: lead, 1: bass, 2: drum, 3: guitar, 4: piano, 5: string, 6: chord

INSTR_MAP = {
    "lead":   ["lead", "synth lead", "pluck", "solo", "melody"],
    "bass":   ["bass", "synth bass"],
    "drum":   ["drum", "percussion", "kit"],
    "guitar": ["guitar", "acoustic guitar", "electric guitar", "ukulele", "mandolin"],
    "piano":  ["piano", "keyboard"],
    "string": ["string", "strings", "violin", "viola", "cello"],
    "chord":  ["pad", "harmony", "backing", "comp", "rhodes", "organ", "chord"]
}

def family_to_track(fam: str):
    """Heuristic mapping from Slakh metadata instrument description to one of the 7 tracks."""
    fam = (fam or "").lower()
    for track_name, keywords in INSTR_MAP.items():
        if any(kw in fam for kw in keywords):
            return track_name
    return None

total_discovered = 0
total_created = 0
total_errors = 0

for split in splits:
    print(f"\n==================== Split: {split} ====================")
    split_src = SRC / split
    print(f"[{split}] Source dir: {split_src}")
    if not split_src.exists():
        print(f"[{split}] ‚ùå does not exist, skipping")
        continue

    split_dest = DEST / split
    split_dest.mkdir(parents=True, exist_ok=True)
    print(f"[{split}] Destination dir: {split_dest}")

    # Song dirs that have the full Slakh structure
    song_dirs = []
    dir_count = 0
    missing_meta = 0
    missing_all_src = 0
    missing_stems = 0
    missing_mix = 0

    print(f"[{split}] Scanning for song folders...")
    for d in sorted(split_src.iterdir()):
        if not d.is_dir():
            continue
        dir_count += 1

        # NOTE: all_src.mid is at the TOP LEVEL of the track folder
        meta = d / "metadata.yaml"
        midi_all_src = d / "all_src.mid"   # üîß patched path
        stems_dir = d / "stems"
        mix_flac = d / "mix.flac"

        has_meta = meta.exists()
        has_midi = midi_all_src.exists()
        has_stems = stems_dir.exists()
        has_mix = mix_flac.exists()

        if has_meta and has_midi and has_stems and has_mix:
            song_dirs.append(d)
        else:
            if not has_meta:
                missing_meta += 1
            if not has_midi:
                missing_all_src += 1
            if not has_stems:
                missing_stems += 1
            if not has_mix:
                missing_mix += 1

    print(f"[{split}] Scanned {dir_count} directories.")
    print(f"[{split}] Using {len(song_dirs)} song dirs with full structure.")
    print(f"[{split}] Missing counts -> "
          f"metadata: {missing_meta}, all_src.mid: {missing_all_src}, "
          f"stems/: {missing_stems}, mix.flac: {missing_mix}")

    total_discovered += len(song_dirs)

    created = 0
    errors = 0

    if not song_dirs:
        print(f"[{split}] ‚ö†Ô∏è No valid song dirs found, skipping conversion for this split.")
        continue

    print(f"[{split}] Starting conversion loop over {len(song_dirs)} songs...")

    for idx, song_dir in enumerate(tqdm(sorted(song_dirs), desc=f"Converting {split}", leave=False)):
        # Occasional heartbeat prints
        if idx % 100 == 0:
            print(f"[{split}] Converting {idx}/{len(song_dirs)} -> {song_dir.name}")

        meta_file = song_dir / "metadata.yaml"
        midi_file = song_dir / "all_src.mid"   # üîß patched path

        # 1) Read metadata
        try:
            with open(meta_file, "r") as f:
                meta = yaml.safe_load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è [{split}] Could not read metadata for {song_dir.name}: {e}")
            errors += 1
            continue

        # 2) Read full multitrack MIDI
        try:
            pm = pretty_midi.PrettyMIDI(str(midi_file))
        except Exception as e:
            print(f"‚ö†Ô∏è [{split}] Could not read MIDI for {song_dir.name}: {e}")
            errors += 1
            continue

        # 3) Build new 7-track PrettyMIDI
        new_pm = pretty_midi.PrettyMIDI()

        # Initialize instruments; mark drum as is_drum=True
        tracks = {
            "lead":   pretty_midi.Instrument(program=80),  # generic lead synth
            "bass":   pretty_midi.Instrument(program=32),  # generic bass
            "drum":   pretty_midi.Instrument(program=0, is_drum=True),
            "guitar": pretty_midi.Instrument(program=24),  # generic guitar
            "piano":  pretty_midi.Instrument(program=0),   # acoustic piano
            "string": pretty_midi.Instrument(program=48),  # string ensemble
            "chord":  pretty_midi.Instrument(program=88),  # pad
        }

        # stems_meta is a dict keyed by stem id; values have instrument info
        stems_meta = list(meta.get("stems", {}).values())

        # Map each Slakh instrument to one of the 7 canonical tracks
        for inst_meta, inst_pm in zip(stems_meta, pm.instruments):
            fam_raw = (
                inst_meta.get("instrument")
                or inst_meta.get("family")
                or inst_meta.get("inst_class")
                or inst_meta.get("plugin_name")
                or ""
            )
            tgt = family_to_track(str(fam_raw))
            if tgt is None:
                continue
            tracks[tgt].notes.extend(inst_pm.notes)

        # Add tracks in the exact order expected by getMUSIC:
        # 0) lead, 1) bass, 2) drum, 3) guitar, 4) piano, 5) string, 6) chord
        for name in ["lead", "bass", "drum", "guitar", "piano", "string", "chord"]:
            new_pm.instruments.append(tracks[name])

        out_path = split_dest / f"{song_dir.name}.mid"
        try:
            new_pm.write(str(out_path))
            created += 1
        except Exception as e:
            print(f"‚ö†Ô∏è [{split}] Failed to write {out_path}: {e}")
            errors += 1

    total_created += created
    total_errors += errors
    num_midis_split = len(list(split_dest.glob("*.mid")))
    print(f"[{split}] ‚úÖ Split summary: created {created} MIDIs, files in {split_dest}: {num_midis_split}, errors: {errors}")

# --- Final global summary ---

num_midis_total = sum(len(list((DEST / s).glob('*.mid'))) for s in splits if (DEST / s).exists())

print("\n‚úÖ Global conversion summary")
print("Source root        :", SRC)
print("Destination root   :", DEST)
print("Discovered songs   :", total_discovered)
print("Created MIDI files :", total_created)
print("MIDIs in DEST      :", num_midis_total)
print("Total errors       :", total_errors)

!apt-get install -y rsync

LOCAL_DEST="/content/slakh2100-merged-tracks"
DRIVE_DEST="/content/drive/MyDrive/datasets/slakh2100-merged-tracks"

print("Removing old drive folder‚Ä¶")
!rm -rf "$DRIVE_DEST"

print("Copying with rsync (this shows progress)‚Ä¶")
!rsync -av --progress "$LOCAL_DEST/" "$DRIVE_DEST/"

import random
from pathlib import Path
import pretty_midi

DEST = Path("/content/slakh2100-merged-tracks")

# Choose which split to inspect: "train", "validation", "test", or "omitted"
split = "train"

split_dir = DEST / split
if not split_dir.exists():
    raise RuntimeError(f"Split dir {split_dir} does not exist. Did the conversion run successfully?")

midi_files = sorted(split_dir.glob("*.mid"))
if not midi_files:
    raise RuntimeError(f"No .mid files found in {split_dir}.")

sample_path = random.choice(midi_files)
print(f"Inspecting file from split '{split}': {sample_path.name}")
print(f"Full path: {sample_path}\n")

pm = pretty_midi.PrettyMIDI(str(sample_path))

print(f"Number of tracks (instruments): {len(pm.instruments)}")
if len(pm.instruments) != 7:
    print("‚ö†Ô∏è Expected 7 tracks (lead, bass, drum, guitar, piano, string, chord).")

print("\nTrack summary:")
for idx, inst in enumerate(pm.instruments):
    print(
        f"  Track {idx}: "
        f"program={inst.program:>3}, "
        f"is_drum={inst.is_drum}, "
        f"notes={len(inst.notes)}"
    )

# Optional: verify that at least one track has notes
total_notes = sum(len(inst.notes) for inst in pm.instruments)
print(f"\nTotal notes across all 7 tracks: {total_notes}")
if total_notes == 0:
    print("‚ö†Ô∏è This merged file has zero notes. That would be suspicious.")
else:
    print("‚úÖ Sanity check passed: file contains note data.")

#more sanity check, actually just play the midi
createAudio(sample_path, "/content/testing_sample.wav")

"""Great now we continue with preprocessing after ensuring that files are 6/7 track format

oct processing
- without patch, it only accepts 20% of the slakh trainset (259/1250) which is highkey really pathetic
- with patch (loosen the filters), we take more of the trainset
"""

!ls "/content"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!rm -rf muzic
!git clone https://github.com/microsoft/muzic.git

# Commented out IPython magic to ensure Python compatibility.
# ====== RUN GETMUSIC to_oct.py ON SLAKH ======
# %cd /content/muzic/getmusic
import os, shutil

# -------------------------------
# CONFIGURE PATHS
# -------------------------------
DATA_PATH = "/content/slakh2100-merged-tracks/train"   # <-- your Slakh MIDI root
PROCESSED_DIR = "example_data/processed_slakh"         # <-- output folder

print("DATA_PATH:", DATA_PATH)
print("PROCESSED_DIR:", PROCESSED_DIR)

# -------------------------------
# REMOVE OLD OUTPUT DIR
# -------------------------------
if os.path.exists(PROCESSED_DIR):
    print("Removing existing", PROCESSED_DIR)
    shutil.rmtree(PROCESSED_DIR)

# Do NOT recreate it manually ‚Äî to_oct.py wants it to NOT exist yet.

# -------------------------------
# RUN to_oct.py
# -------------------------------
print("\n‚û°Ô∏è Running preprocess/to_oct.py ...\n")
!python preprocess/to_oct.py "$DATA_PATH" "$PROCESSED_DIR" 2>&1 | tee to_oct_log.txt

# -------------------------------
# CHECK RESULTS
# -------------------------------
print("\n‚û°Ô∏è Listing processed_slakh:")
!ls -la {PROCESSED_DIR}

oct_path = os.path.join(PROCESSED_DIR, "oct.txt")
if not os.path.exists(oct_path):
    print("\n‚ùå oct.txt was NOT created. Showing last 40 lines of to_oct_log.txt:\n")
    !tail -n 40 to_oct_log.txt
else:
    print("\n‚úÖ oct.txt exists at:", oct_path)

"""Step 2: Build Dictionary + Patch midi_config.py"""

# Commented out IPython magic to ensure Python compatibility.
# ============= STEP 2 ‚Äî MAKE DICTIONARY =============
# %cd /content/muzic/getmusic
import ast, re, pathlib

# Make sure PROCESSED_DIR matches Step 1
PROCESSED_DIR = "example_data/processed_slakh"
print("Using PROCESSED_DIR:", PROCESSED_DIR)

# --------------------------------------
# Run make_dict.py on preprocessed data
# --------------------------------------
print("\n‚û°Ô∏è Running make_dict.py ...\n")

# num_threshold=3 is typical; change if needed
!python preprocess/make_dict.py {PROCESSED_DIR}/ 3 2>&1 | tee make_dict_log.txt

# --------------------------------------
# Extract tracks_start / tracks_end lists
# --------------------------------------
log_lines = open("make_dict_log.txt", "r").read().splitlines()

# Lines printed by make_dict at the end look like:
#   [dur_vocab_size, ...]
#   [end_ls...]
list_lines = [ln for ln in log_lines if ln.strip().startswith('[') and ln.strip().endswith(']')]

if len(list_lines) < 2:
    print("‚ùå ERROR: Could not find tracks_start/tracks_end in output!")
    print("Last 40 log lines:\n")
    print("\n".join(log_lines[-40:]))
    raise RuntimeError("make_dict.py failed to output tracks_start/tracks_end.")

tracks_start = ast.literal_eval(list_lines[-2])
tracks_end   = ast.literal_eval(list_lines[-1])

print("\nExtracted:")
print("tracks_start =", tracks_start)
print("tracks_end   =", tracks_end)

# --------------------------------------
# Patch midi_config.py with new ranges
# --------------------------------------
mc_path = pathlib.Path("getmusic/utils/midi_config.py")
text = mc_path.read_text()

print("\n‚û°Ô∏è Patching getmusic/utils/midi_config.py ...")

# Replace existing array values
text = re.sub(r"tracks_start\s*=\s*\[.*?\]", f"tracks_start = {tracks_start}", text)
text = re.sub(r"tracks_end\s*=\s*\[.*?\]", f"tracks_end = {tracks_end}", text)

mc_path.write_text(text)

print("\n‚úÖ Updated getmusic/utils/midi_config.py with new token ID ranges.")
print("You may now proceed to Step 3: binarize.")

# --------------------------------------
# Show confirmation
# --------------------------------------
patched = pathlib.Path("getmusic/utils/midi_config.py").read_text()
print("\n----- PATCHED midi_config.py (excerpt) -----")
for line in patched.split("\n"):
    if "tracks_start" in line or "tracks_end" in line:
        print(line)
print("---------------------------------------------")

"""Step 3: binarize
-first patch it so that MIDI program 25 gets read as 24 (they're just different guitars)

"""

#CLEAR THE PATCH WITH UPSTREAM VERSION
!wget -O /content/muzic/getmusic/preprocess/binarize.py \
  https://raw.githubusercontent.com/microsoft/muzic/master/getmusic/preprocess/binarize.py

# Commented out IPython magic to ensure Python compatibility.
#PATCH FOR INSTRUMENT PROGRAMS 24, 88
# %cd /content/muzic/getmusic
from pathlib import Path

bpath = Path("preprocess/binarize.py")
text = bpath.read_text()

# 1) Extend prog_to_abrv: add 24 as guitar ("G"), 88 as lead/pad ("M")
text = text.replace(
    "prog_to_abrv = {'80':'M', '32':'B', '128':'D', '25':'G', '0':'P', '48':'S',}",
    "prog_to_abrv = {'80':'M', '32':'B', '128':'D', '25':'G', '24':'G', '0':'P', '48':'S', '88':'M',}"
)

# 2) Extend inst_to_row: map 24 to guitar row (3), 88 to melody/pad row (0)
text = text.replace(
    "inst_to_row = {'80':0, '32':1, '128':2, '25':3, '0':4, '48':5, '129':6}",
    "inst_to_row = {'80':0, '32':1, '128':2, '25':3, '24':3, '0':4, '48':5, '88':0, '129':6}"
)

# 3) Extend no_empty_tracks inside oct_to_rep so 24/88 don't KeyError
text = text.replace(
    "    no_empty_tracks = {'80':0,'32':0,'128':0,'25':0,'0':0,'48':0}\n",
    "    no_empty_tracks = {'80':0,'32':0,'128':0,'25':0,'24':0,'0':0,'48':0,'88':0}\n"
)

bpath.write_text(text)
print("‚úÖ Updated prog_to_abrv, inst_to_row, and no_empty_tracks to support programs 24 and 88.")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import os

print("Using PROCESSED_DIR:", PROCESSED_DIR)

pitch_dict_path = os.path.join(PROCESSED_DIR, "pitch_dict.txt")
if not os.path.exists(pitch_dict_path):
    raise FileNotFoundError(f"{pitch_dict_path} not found; make_dict may have failed.")

!python preprocess/binarize.py \
  {pitch_dict_path} \
  {PROCESSED_DIR}/oct.txt \
  {PROCESSED_DIR}

print("\nContents of", PROCESSED_DIR, "after binarize:")
!ls -la {PROCESSED_DIR}

import shutil, os
from pathlib import Path

SRC = Path("/content/muzic/getmusic/example_data/processed_slakh")
DST = Path("/content/drive/MyDrive/datasets/processed_slakh")

print("Backing up processed_slakh ->", DST)
if DST.exists():
    print("Removing old backup...")
    shutil.rmtree(DST)

shutil.copytree(SRC, DST)
print("‚úÖ Backup complete.")

"""PREPROCESSING IS DONE, BACKED UP TO DRIVE
NOW WE PROCEED TO TRAINING
"""

print("Using PROCESSED_DIR =", PROCESSED_DIR)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import pathlib, re

TRAIN_YAML = pathlib.Path("configs/train.yaml")
text = TRAIN_YAML.read_text()

# Compute vocab_size = (#lines in pitch_dict.txt) + 1
pitch_dict_path = pathlib.Path(PROCESSED_DIR) / "pitch_dict.txt"
num_tokens = sum(1 for _ in pitch_dict_path.open("r", encoding="utf-8") if _.strip())
vocab_size = num_tokens + 1

print("Tokens in pitch_dict.txt:", num_tokens)
print("=> vocab_size we will set:", vocab_size)

# 1) Update ALL vocab_size occurrences
text = re.sub(r"vocab_size:\s*\d+", f"vocab_size: {vocab_size}", text)

# 2) Update vocab_path
# (only first match should be the solver's vocab_path)
text = re.sub(r"vocab_path:\s*\S+", f"vocab_path: {PROCESSED_DIR}/pitch_dict.txt", text, count=1)

# 3) Update all data_folder occurrences (train + valid)
text = re.sub(r"data_folder:\s*\S+", f"data_folder: {PROCESSED_DIR}", text)

# 4) Also replace any '/your-data-path' leftovers, just in case
text = text.replace("/your-data-path", PROCESSED_DIR)

TRAIN_YAML.write_text(text)
print("\n‚úÖ Patched configs/train.yaml (vocab_size, vocab_path, data_folder).")

print("\nSanity check for /your-data-path:")
!grep -R "your-data-path" -n configs || echo "No /your-data-path remains in configs."

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import pathlib, re

tp = pathlib.Path("train.py")
text = tp.read_text()

# We look for the line `solver.train()` in main_worker and append a manual save right after it.
pattern = "    solver = Solver(config=config, args=args, model=model, dataloader=dataloader_info, logger=logger)\n" \
          "    solver.train()\n"

replacement = (
    "    solver = Solver(config=config, args=args, model=model, dataloader=dataloader_info, logger=logger)\n"
    "    solver.train()\n"
    "    # --- manual checkpoint save added for Colab ---\n"
    "    import torch, os\n"
    "    os.makedirs('OUTPUT', exist_ok=True)\n"
    "    torch.save(model.state_dict(), 'OUTPUT/manual_final.pth')\n"
)

if pattern not in text:
    # Fallback: only replace the `solver.train()` line, keeping indentation
    text = text.replace(
        "    solver.train()\n",
        "    solver.train()\n"
        "    # --- manual checkpoint save added for Colab (fallback) ---\n"
        "    import torch, os\n"
        "    os.makedirs('OUTPUT', exist_ok=True)\n"
        "    torch.save(model.state_dict(), 'OUTPUT/manual_final.pth')\n"
    )
    print("‚ö†Ô∏è Pattern not matched exactly; used fallback replacement.")
else:
    text = text.replace(pattern, replacement)
    print("‚úÖ Patched train.py to save OUTPUT/manual_final.pth after training.")

tp.write_text(text)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import pathlib, re

TRAIN_YAML = pathlib.Path("configs/train.yaml")
text = TRAIN_YAML.read_text()

# Add checkpoint-related keys under 'solver:' if not already present
if "save_checkpoints:" not in text:
    # Insert right after the 'solver:' line
    text = re.sub(
        r'^solver:\s*$',           # a line that is exactly "solver:"
        'solver:\n'
        '  save_checkpoints: True\n'
        '  checkpoint_dir: OUTPUT/checkpoints\n'
        '  save_epochs: 1',
        text,
        count=1,
        flags=re.MULTILINE,
    )
    TRAIN_YAML.write_text(text)
    print("‚úÖ Added save_checkpoints / checkpoint_dir / save_epochs under solver.")
else:
    print("‚ÑπÔ∏è Checkpoint fields already present in configs/train.yaml.")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
!python train.py

# %cd /content/muzic/getmusic
import torch, os

raw_path = "OUTPUT/manual_final.pth"
if not os.path.exists(raw_path):
    raise FileNotFoundError(f"{raw_path} not found. Did train.py finish and save it?")

# This is the pure model.state_dict() we saved in train.py
state_dict = torch.load(raw_path, map_location="cpu")

# Wrap in the format Solver.resume() expects
wrapped = {
    "last_epoch": 0,
    "global_step": 0,
    "state_dict": state_dict,
    # These won't be used because track_generation sets no_load_optimizer_and_scheduler=True,
    # but we include them for completeness.
    "optimizer": {},
    "scheduler": {},
}

wrapped_path = "OUTPUT/manual_solver.pth"
torch.save(wrapped, wrapped_path)
print("‚úÖ Wrote wrapped checkpoint:", wrapped_path)

from google.colab import drive
drive.mount('/content/drive')

import os
import shutil

# Define the source directory where your .pth files are located
source_dir = "/content/muzic/getmusic/OUTPUT"

# Define the destination folder in your Google Drive
# You can change 'GETMusic_Checkpoints' to any folder name you prefer.
# The path starts from '/content/drive/MyDrive/' after mounting.
destination_folder = "/content/drive/MyDrive/Colab Notebooks/GETMusic_Checkpoints"

# Create the destination folder in Google Drive if it doesn't exist
os.makedirs(destination_folder, exist_ok=True)

# Paths to the files you want to copy
file_to_copy1 = os.path.join(source_dir, "manual_final.pth")
file_to_copy2 = os.path.join(source_dir, "manual_solver.pth")

# Copy manual_final.pth
if os.path.exists(file_to_copy1):
    shutil.copy(file_to_copy1, destination_folder)
    print(f"‚úÖ Copied {os.path.basename(file_to_copy1)} to {destination_folder}")
else:
    print(f"‚ùå {os.path.basename(file_to_copy1)} not found in {source_dir}")

# Copy manual_solver.pth
if os.path.exists(file_to_copy2):
    shutil.copy(file_to_copy2, destination_folder)
    print(f"‚úÖ Copied {os.path.basename(file_to_copy2)} to {destination_folder}")
else:
    print(f"‚ùå {os.path.basename(file_to_copy2)} not found in {source_dir}")

print("\nDone copying checkpoints to Google Drive.")

import os
import shutil
from pathlib import Path

# Paths
SRC = Path("/content/muzic/getmusic/OUTPUT")
DST_ROOT = Path("/content/drive/MyDrive/datasets")
DST = DST_ROOT / "getmusic_OUTPUT"

print("Source OUTPUT dir :", SRC)
print("Drive target dir  :", DST)

# Sanity checks
if not SRC.exists():
    raise RuntimeError("‚ùå Source OUTPUT directory does not exist yet. Did training run and save anything?")

DST_ROOT.mkdir(parents=True, exist_ok=True)

# Remove old backup if it exists
if DST.exists():
    print("‚ö†Ô∏è Existing getmusic_OUTPUT found on Drive. Removing it first...")
    shutil.rmtree(DST)

# Copy everything
print("üì¶ Copying OUTPUT -> Drive (this may take a bit)...")
shutil.copytree(SRC, DST)

print("‚úÖ Done! OUTPUT is now backed up at:", DST)









#IGNORE STARTING BELOW HERE THE CODE IS BOOTYCHEEKS
#IGNORE!!!!!!

# Commented out IPython magic to ensure Python compatibility.
# @title
#DEPRECATED, DO NOT USE THIS CELL
#PROPOSED PATCH NUMBER 2
# %cd /content/muzic/getmusic
from pathlib import Path

solver_path = Path("getmusic/engine/solver.py")
text = solver_path.read_text()

patch = r"""

# === Patched resume to support multiple checkpoint formats ===
def _patched_resume(self, path):
    \"\"\"Resume from a checkpoint.

    Supports:
    - training-style checkpoints: {'last_epoch', 'global_step', 'state_dict', ...}
    - plain model state_dicts: {...weights...}
    \"\"\"
    import torch

    ckpt = torch.load(path, map_location='cpu')

    # Case 1: dict with 'state_dict' -> training checkpoint
    if isinstance(ckpt, dict) and 'state_dict' in ckpt:
        state_dict = ckpt['state_dict']
        self.last_epoch = ckpt.get('last_epoch', 0)
        self.global_step = ckpt.get('global_step', 0)

        self.model.load_state_dict(state_dict)

        # Best-effort loading of optimizer/scheduler if they exist and are allowed
        if not getattr(self.args, "no_load_optimizer_and_scheduler", False):
            opt_state = ckpt.get('optimizer')
            sch_state = ckpt.get('scheduler')
            if opt_state is not None and hasattr(self, "optimizer"):
                try:
                    self.optimizer.load_state_dict(opt_state)
                except Exception:
                    pass
            if sch_state is not None and hasattr(self, "scheduler"):
                try:
                    self.scheduler.load_state_dict(sch_state)
                except Exception:
                    pass

    # Case 2: plain state_dict (or weird nested dict)
    else:
        self.last_epoch = 0
        self.global_step = 0
        loaded = False

        # Try treating ckpt directly as state_dict
        try:
            self.model.load_state_dict(ckpt)
            loaded = True
        except Exception:
            pass

        # If that fails, try each value (e.g., {'model': {...}})
        if not loaded and isinstance(ckpt, dict):
            for v in ckpt.values():
                try:
                    self.model.load_state_dict(v)
                    loaded = True
                    break
                except Exception:
                    continue

    self.print_log(f"Resumed checkpoint from {path}, "
                   f"last_epoch={getattr(self, 'last_epoch', 0)}, "
                   f"global_step={getattr(self, 'global_step', 0)}")

# Override the original resume with the patched one
Solver.resume = _patched_resume
# === End patched resume ===
"""

# Only append once
if "_patched_resume(self, path)" not in text:
    text = text + "\n" + patch
    solver_path.write_text(text)
    print("‚úÖ Appended patched Solver.resume to getmusic/engine/solver.py")
else:
    print("‚ÑπÔ∏è Patched resume already present in solver.py")

# @title
!python track_generation.py \
  --load_path OUTPUT/manual_solver.pth \
  --file_path example_data/inference

# Commented out IPython magic to ensure Python compatibility.
# @title
#DEPRECATED, DO NOT USE THIS CELL
# %cd /content/muzic/getmusic
from pathlib import Path
import re

solver_path = Path("getmusic/engine/solver.py")
text = solver_path.read_text()

# Remove anything after our injected comment marker if present
clean_text = re.sub(
    r"REPLACE THIS SHIT*",
    "",
    text
)

solver_path.write_text(clean_text)
print("‚úÖ Removed previous broken patch from solver.py")











# Commented out IPython magic to ensure Python compatibility.
# @title
# If you want GPU: Runtime -> Change runtime type -> GPU

!rm -rf /content/muzic
# %cd /content

# Clone Muzic
!git clone https://github.com/microsoft/muzic.git
# %cd /content/muzic/getmusic
!ls

# Install dependencies compatible with GETMusic
!pip install -q "torch==1.12.1" "torchvision==0.13.1" "torchaudio==0.12.1"
!pip install -q tensorboard pyyaml tqdm transformers einops miditoolkit scipy

# Commented out IPython magic to ensure Python compatibility.
# @title
# %cd /content/muzic/getmusic
import pathlib

lr_path = pathlib.Path("getmusic/engine/lr_scheduler.py")
text = lr_path.read_text()

if "torch._six" in text:
    text = text.replace("from torch._six import inf",
                        "from math import inf  # patched: torch._six removed in newer Torch")
    lr_path.write_text(text)
    print("‚úÖ Patched getmusic/engine/lr_scheduler.py (torch._six ‚Üí math.inf)")
else:
    print("‚ÑπÔ∏è lr_scheduler.py already patched or uses a different import.")

# Commented out IPython magic to ensure Python compatibility.
# @title
# %cd /content/muzic/getmusic
import os, time

# Choose a processed directory; we will let to_oct.py create it
PROCESSED_DIR = "example_data/processed_demo"
print("Using PROCESSED_DIR:", PROCESSED_DIR)

# Make sure it does not exist before running to_oct
!rm -rf {PROCESSED_DIR}

print("Train MIDIs that will be processed:")
!ls example_data/train

print("\nRunning preprocess/to_oct.py ...\n")
!python preprocess/to_oct.py example_data/train {PROCESSED_DIR} | tee to_oct_log.txt

print("\nContents of", PROCESSED_DIR, ":")
!ls -la {PROCESSED_DIR}

oct_path = os.path.join(PROCESSED_DIR, "oct.txt")
if not os.path.exists(oct_path):
    print("\n‚ùå oct.txt was NOT created. Last 40 lines of to_oct_log.txt:")
    !tail -n 40 to_oct_log.txt
else:
    print("\n‚úÖ oct.txt exists at", oct_path)

# Commented out IPython magic to ensure Python compatibility.
# @title
# %cd /content/muzic/getmusic
import ast, re, pathlib

print("Using PROCESSED_DIR:", PROCESSED_DIR)

# Build the dictionary and capture log
!python preprocess/make_dict.py {PROCESSED_DIR}/ 3 | tee make_dict_log.txt

log_lines = open("make_dict_log.txt", "r").read().splitlines()
list_lines = [ln for ln in log_lines if ln.strip().startswith('[') and ln.strip().endswith(']')]

if len(list_lines) < 2:
    raise RuntimeError("Could not find tracks_start/tracks_end arrays in make_dict output.")

start_ls = ast.literal_eval(list_lines[-2])
end_ls   = ast.literal_eval(list_lines[-1])

print("tracks_start:", start_ls)
print("tracks_end  :", end_ls)

# Patch midi_config.py
mc_path = pathlib.Path("getmusic/utils/midi_config.py")
text = mc_path.read_text()

text = re.sub(r"tracks_start\s*=\s*\[.*?\]", f"tracks_start = {start_ls}", text)
text = re.sub(r"tracks_end\s*=\s*\[.*?\]", f"tracks_end = {end_ls}", text)

mc_path.write_text(text)
print("\n‚úÖ Updated getmusic/utils/midi_config.py with new tracks_start/tracks_end.")

# Commented out IPython magic to ensure Python compatibility.
# @title
# %cd /content/muzic/getmusic
import os

print("Using PROCESSED_DIR:", PROCESSED_DIR)

pitch_dict_path = os.path.join(PROCESSED_DIR, "pitch_dict.txt")
if not os.path.exists(pitch_dict_path):
    raise FileNotFoundError(f"{pitch_dict_path} not found; make_dict may have failed.")

!python preprocess/binarize.py \
  {pitch_dict_path} \
  {PROCESSED_DIR}/oct.txt \
  {PROCESSED_DIR}

print("\nContents of", PROCESSED_DIR, "after binarize:")
!ls -la {PROCESSED_DIR}

# Commented out IPython magic to ensure Python compatibility.
# @title
# %cd /content/muzic/getmusic
import pathlib, yaml

print("Using PROCESSED_DIR:", PROCESSED_DIR)
pitch_dict_path = pathlib.Path(PROCESSED_DIR) / "pitch_dict.txt"
if not pitch_dict_path.exists():
    raise FileNotFoundError(f"{pitch_dict_path} not found.")

# Just for info; we won't overwrite vocab_size
num_tokens = sum(1 for _ in pitch_dict_path.open("r", encoding="utf-8"))
vocab_size = num_tokens + 1
print("Tokens in pitch_dict.txt:", num_tokens)
print("=> vocab_size (info only):", vocab_size)

# Reset train.yaml from repo
!git checkout -- configs/train.yaml

yaml_path = pathlib.Path("configs/train.yaml")
cfg = yaml.load(yaml_path.read_text(), Loader=yaml.FullLoader)

def patch_config(node, path=""):
    if isinstance(node, dict):
        for k, v in list(node.items()):
            new_path = f"{path}.{k}" if path else k
            if isinstance(v, (dict, list)):
                patch_config(v, new_path)
            # Patch data_dir
            if k == "data_dir":
                print(f"patching data_dir at {new_path}: {v} -> {PROCESSED_DIR}")
                node[k] = PROCESSED_DIR
            # Patch vocab_path
            if k == "vocab_path":
                print(f"patching vocab_path at {new_path}: {v} -> {pitch_dict_path}")
                node[k] = str(pitch_dict_path)
    elif isinstance(node, list):
        for i, v in enumerate(node):
            patch_config(v, f"{path}[{i}]")

patch_config(cfg)

yaml_path.write_text(yaml.dump(cfg, sort_keys=False))
print("\n‚úÖ Patched configs/train.yaml (data_dir ‚Üí processed dir, vocab_path ‚Üí pitch_dict.txt).")

# Commented out IPython magic to ensure Python compatibility.
# @title
# %cd /content/muzic/getmusic
!grep -R "your-data-path" -n . || echo "‚úÖ No /your-data-path placeholder found."

# @title
PROCESSED_DIR="example_data/processed_demo"
!sed -i "s#/your-data-path#$PROCESSED_DIR#g" configs/train.yaml getmusic/data/bigdata.py || true

# Commented out IPython magic to ensure Python compatibility.
# @title
# %cd /content/muzic/getmusic
!python train.py

# @title
import pathlib, re, os, yaml

TRAIN_YAML = pathlib.Path("/content/muzic/getmusic/configs/train.yaml")

# 1) Read raw text and normalize the !!python/tuple to a plain list
raw = TRAIN_YAML.read_text()
raw = re.sub(r"betas:\s*!!python/tuple\s*\[0\.9,\s*0\.999\]",
             "betas: [0.9, 0.999]",
             raw)

cfg = yaml.safe_load(raw)

# --- You should already have these from the preprocessing step ---
PROCESSED_DIR = pathlib.Path("/content/muzic/getmusic/example_data/processed_demo_1765006471")
vocab_path = PROCESSED_DIR / "pitch_dict.txt"

# Compute vocab_size = #tokens + 1 (for [EMPTY])
with open(vocab_path) as f:
    num_tokens = len([line for line in f if line.strip()])
vocab_size = num_tokens + 1

print("Using processed dir:", PROCESSED_DIR)
print("Tokens in pitch_dict.txt:", num_tokens)
print("=> vocab_size:", vocab_size)

# 2) Patch **model** section
cfg["model"]["params"]["diffusion_config"]["params"]["roformer_config"]["params"]["vocab_size"] = vocab_size

# 3) Patch **solver**: vocab and checkpoint options
solver = cfg["solver"]

solver["vocab_path"] = str(vocab_path)

# Enable checkpoints + where to store them
solver["save_checkpoints"] = True
solver["checkpoint_dir"] = "OUTPUT/checkpoints"
# Save a checkpoint every epoch to be safe
solver["save_epochs"] = 1

# 4) Patch **dataloader** paths & vocab_size
dl = cfg["dataloader"]
dl["batch_size"] = 1  # safer for Colab

train_ds = dl["train_datasets"][0]["params"]
valid_ds = dl["validation_datasets"][0]["params"]

train_ds["path"] = str(PROCESSED_DIR)
valid_ds["path"] = str(PROCESSED_DIR)
train_ds["vocab_size"] = vocab_size
valid_ds["vocab_size"] = vocab_size

# 5) Write back YAML
TRAIN_YAML.write_text(yaml.dump(cfg, sort_keys=False))

print("\nPatched train.yaml:")
print("  solver.vocab_path      =", cfg["solver"]["vocab_path"])
print("  solver.save_checkpoints=", cfg["solver"]["save_checkpoints"])
print("  solver.checkpoint_dir  =", cfg["solver"]["checkpoint_dir"])
print("  solver.save_epochs     =", cfg["solver"]["save_epochs"])
print("  train path             =", train_ds["path"])
print("  valid path             =", valid_ds["path"])
print("  vocab_size             =", vocab_size)

# Commented out IPython magic to ensure Python compatibility.
# @title
# %cd /content/muzic/getmusic

# Show everything under OUTPUT (paths + filenames)
!find OUTPUT -maxdepth 5 -type f -print || echo "No files under OUTPUT yet."





















"""Kaggle stuff? get MIDI dataset"""

# Commented out IPython magic to ensure Python compatibility.
# Run this in a FRESH runtime
!rm -rf /content/muzic
# %cd /content

# Clone Muzic
!git clone https://github.com/microsoft/muzic.git
# %cd /content/muzic/getmusic
!ls

# Install deps
!pip install -q "torch==1.12.1" "torchvision==0.13.1" "torchaudio==0.12.1"
!pip install -q tensorboard pyyaml tqdm transformers einops miditoolkit scipy kaggle

# Commented out IPython magic to ensure Python compatibility.
import os, glob
from google.colab import files

# Kaggle setup
kaggle_dir = os.path.expanduser("~/.kaggle")
os.makedirs(kaggle_dir, exist_ok=True)

if not os.path.exists(os.path.join(kaggle_dir, "kaggle.json")):
    print("Upload kaggle.json (Kaggle -> Account -> Create New API Token):")
    uploaded = files.upload()
    if "kaggle.json" not in uploaded:
        raise RuntimeError("Please upload kaggle.json")
    with open(os.path.join(kaggle_dir, "kaggle.json"), "wb") as f:
        f.write(uploaded["kaggle.json"])
    !chmod 600 ~/.kaggle/kaggle.json

# Download dataset
# %cd /content
!kaggle datasets download -d soumikrakshit/classical-music-midi -p /content -q

# Unzip
!mkdir -p /content/kaggle_data/classical_midi
!unzip -o /content/classical-music-midi.zip -d /content/kaggle_data/classical_midi > /dev/null

# Collect all midis (.mid / .MID)
midis = sorted(set(
    glob.glob("/content/kaggle_data/classical_midi/**/*.mid", recursive=True) +
    glob.glob("/content/kaggle_data/classical_midi/**/*.MID", recursive=True)
))

print(f"Found {len(midis)} Kaggle MIDIs.")
print("Sample:", midis[:5])

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import os, shutil, glob

# Inference directory for track_generation.py
INFER_DIR = "example_data/inference"
os.makedirs(INFER_DIR, exist_ok=True)

# Workspace mirror (like /Users/mihikadusad/MIDI_experiments locally)
MIDI_EXP_DIR = "/content/MIDI_experiments"
os.makedirs(MIDI_EXP_DIR, exist_ok=True)

# Copy a subset of Kaggle MIDIs into inference + workspace
all_midis = sorted(set(
    glob.glob("/content/kaggle_data/classical_midi/**/*.mid", recursive=True) +
    glob.glob("/content/kaggle_data/classical_midi/**/*.MID", recursive=True)
))
N_INFER = min(30, len(all_midis))
infer_midis = all_midis[:N_INFER]

for src in infer_midis:
    base = os.path.basename(src)
    shutil.copy(src, os.path.join(INFER_DIR, base))
    shutil.copy(src, os.path.join(MIDI_EXP_DIR, base))

print("Built-in demo train MIDIs:")
!ls example_data/train
print("\nInference MIDIs (Kaggle subset):")
!ls example_data/inference | head

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import os, time

# Brand new processed dir name
PROCESSED_DIR = f"example_data/processed_demo_{int(time.time())}"

print("Using processed dir:", PROCESSED_DIR)
print("Ensuring it does NOT exist...")
!rm -rf {PROCESSED_DIR}

# IMPORTANT: do NOT os.makedirs(PROCESSED_DIR) here ‚Äì to_oct.py will do it

print("\nRunning to_oct.py ...\n")
!python preprocess/to_oct.py example_data/train {PROCESSED_DIR} | tee to_oct_log.txt

print("\nContents of", PROCESSED_DIR, ":")
!ls -la {PROCESSED_DIR} || echo "Directory missing?"

oct_path = os.path.join(PROCESSED_DIR, "oct.txt")
if not os.path.exists(oct_path):
    print("\n‚ùå oct.txt was NOT created. Last 40 lines of to_oct_log.txt:")
    !tail -n 40 to_oct_log.txt
else:
    print("\n‚úÖ oct.txt exists at", oct_path)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import ast, re, pathlib

# Use the same PROCESSED_DIR as above ‚Äì rerun that cell first in this runtime
# If you restarted runtime, re-run cell 3 to re-define PROCESSED_DIR.
print("Using processed dir:", PROCESSED_DIR)

# Build pitch_dict + get tracks_start/end
!python preprocess/make_dict.py {PROCESSED_DIR}/ 3 | tee make_dict_log.txt

log_lines = open("make_dict_log.txt", "r").read().splitlines()
list_lines = [ln for ln in log_lines if ln.strip().startswith('[') and ln.strip().endswith(']')]
if len(list_lines) < 2:
    raise RuntimeError("Could not find start_ls/end_ls in make_dict output.")

start_ls = ast.literal_eval(list_lines[-2])
end_ls   = ast.literal_eval(list_lines[-1])

print("tracks_start:", start_ls)
print("tracks_end  :", end_ls)

# Patch getmusic/utils/midi_config.py
mc_path = pathlib.Path("getmusic/utils/midi_config.py")
text = mc_path.read_text()

text = re.sub(r"tracks_start\s*=\s*\[.*?\]", f"tracks_start = {start_ls}", text)
text = re.sub(r"tracks_end\s*=\s*\[.*?\]", f"tracks_end = {end_ls}", text)

mc_path.write_text(text)
print("\n‚úÖ Updated midi_config.py")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import pathlib, re

print("Using processed dir:", PROCESSED_DIR)

pitch_dict_path = pathlib.Path(PROCESSED_DIR) / "pitch_dict.txt"
num_tokens = sum(1 for _ in pitch_dict_path.open("r", encoding="utf-8"))
vocab_size = num_tokens + 1

print("Tokens in pitch_dict.txt:", num_tokens)
print("=> vocab_size:", vocab_size)

yaml_path = pathlib.Path("configs/train.yaml")
text = yaml_path.read_text()

# Update all vocab_size entries
text = re.sub(r"vocab_size:\s*\d+", f"vocab_size: {vocab_size}", text)

# Set vocab_path
text = re.sub(r"vocab_path:\s*\S+", f"vocab_path: {PROCESSED_DIR}/pitch_dict.txt", text)

# Set all dataset paths to PROCESSED_DIR
text = re.sub(r"path:\s*[^ \n]+", f"path: {PROCESSED_DIR}", text)

yaml_path.write_text(text)
print("\n‚úÖ Patched configs/train.yaml")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic

print("Using processed dir:", PROCESSED_DIR)

!python preprocess/binarize.py \
  {PROCESSED_DIR}/pitch_dict.txt \
  {PROCESSED_DIR}/oct.txt \
  {PROCESSED_DIR}

print("\nContents of", PROCESSED_DIR, "after binarize:")
!ls -la {PROCESSED_DIR}

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic

!python train.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import pathlib

lr_path = pathlib.Path("getmusic/engine/lr_scheduler.py")
text = lr_path.read_text()

old = "from torch._six import inf"
new = "from math import inf  # patched: torch._six removed in newer PyTorch"

if old not in text:
    print("The expected import line is not present; showing first 40 lines for inspection:")
    print("\n".join(text.splitlines()[:40]))
else:
    text = text.replace(old, new)
    lr_path.write_text(text)
    print("‚úÖ Patched getmusic/engine/lr_scheduler.py to use math.inf instead of torch._six.inf")



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import os, pathlib, yaml

print("Using processed dir:", PROCESSED_DIR)

pitch_dict_path = pathlib.Path(PROCESSED_DIR) / "pitch_dict.txt"
if not pitch_dict_path.exists():
    raise FileNotFoundError(f"pitch_dict.txt not found at {pitch_dict_path}")

# Just for info
num_tokens = sum(1 for _ in pitch_dict_path.open("r", encoding="utf-8"))
vocab_size = num_tokens + 1
print("Tokens in pitch_dict.txt:", num_tokens)
print("=> vocab_size (info only):", vocab_size)

# 1) Restore original train.yaml
!git checkout -- configs/train.yaml

yaml_path = pathlib.Path("configs/train.yaml")
text = yaml_path.read_text()

# 2) Load with a loader that supports !!python/tuple
cfg = yaml.load(text, Loader=yaml.FullLoader)

# 3) Recursively patch any 'data_dir' and 'vocab_path' keys
def patch_config(node, path=""):
    if isinstance(node, dict):
        for k, v in list(node.items()):
            new_path = f"{path}.{k}" if path else k
            if isinstance(v, (dict, list)):
                patch_config(v, new_path)
            # patch data_dir
            if k == "data_dir":
                print(f"patching data_dir at {new_path}: {v} -> {PROCESSED_DIR}")
                node[k] = PROCESSED_DIR
            # patch vocab_path
            if k == "vocab_path":
                print(f"patching vocab_path at {new_path}: {v} -> {pitch_dict_path}")
                node[k] = str(pitch_dict_path)
    elif isinstance(node, list):
        for i, v in enumerate(node):
            patch_config(v, f"{path}[{i}]")

patch_config(cfg)

yaml_path.write_text(yaml.dump(cfg, sort_keys=False))
print("\n‚úÖ train.yaml patched: all data_dir -> processed dir, all vocab_path -> pitch_dict.txt.")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import glob, os, shutil

pth_files = glob.glob("OUTPUT/**/*.pth", recursive=True)
print("Found checkpoints:", pth_files)

if not pth_files:
    raise RuntimeError("No .pth checkpoints found. Check train.py output.")

latest_ckpt = max(pth_files, key=os.path.getmtime)
print("Using latest checkpoint:", latest_ckpt)

shutil.copy(latest_ckpt, "checkpoint_trained_demo.pth")
print("‚úÖ Copied to checkpoint_trained_demo.pth")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
!ls -R

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
!grep -R "your-data-path" -n .

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic

PROCESSED_DIR="example_data/processed_demo_1765006471"

!grep -R "your-data-path" -n .

# Replace in ALL files under getmusic/
!sed -i "s#/your-data-path#$PROCESSED_DIR#g" configs/train.yaml || true
!sed -i "s#/your-data-path#$PROCESSED_DIR#g" config/train.yaml || true
!sed -i "s#/your-data-path#$PROCESSED_DIR#g" getmusic/data/bigdata.py || true

# Show what changed
!grep -R "processed_demo_1765" -n configs train* getmusic/data || true







#### IGNORE ALL BELOW CELLS

# Commented out IPython magic to ensure Python compatibility.
# NEW CLEAN START
!rm -rf /content/muzic
# %cd /content

# Clone Muzic
!git clone https://github.com/microsoft/muzic.git
# %cd /content/muzic/getmusic
!ls

# Install requirements for GETMusic (CPU/GPU ok)
!pip install -q "torch==1.12.1" "torchvision==0.13.1" "torchaudio==0.12.1"
!pip install -q tensorboard pyyaml tqdm transformers einops miditoolkit scipy kaggle

# Commented out IPython magic to ensure Python compatibility.
import os, glob
from google.colab import files

# Kaggle API config
kaggle_dir = os.path.expanduser("~/.kaggle")
os.makedirs(kaggle_dir, exist_ok=True)

if not os.path.exists(os.path.join(kaggle_dir, "kaggle.json")):
    print("Upload kaggle.json (from Kaggle -> Account -> Create New API Token):")
    uploaded = files.upload()
    if "kaggle.json" not in uploaded:
        raise RuntimeError("Please upload kaggle.json")
    with open(os.path.join(kaggle_dir, "kaggle.json"), "wb") as f:
        f.write(uploaded["kaggle.json"])
    !chmod 600 ~/.kaggle/kaggle.json

# Download dataset
# %cd /content
!kaggle datasets download -d soumikrakshit/classical-music-midi -p /content -q

# Unzip
!mkdir -p /content/kaggle_data/classical_midi
!unzip -o /content/classical-music-midi.zip -d /content/kaggle_data/classical_midi > /dev/null

# Find MIDIs (both .mid and .MID)
midis_lower = glob.glob("/content/kaggle_data/classical_midi/**/*.mid", recursive=True)
midis_upper = glob.glob("/content/kaggle_data/classical_midi/**/*.MID", recursive=True)
all_midis = sorted(set(midis_lower + midis_upper))

print(f"Found {len(all_midis)} MIDI files in Kaggle dataset.")
print("Sample:", all_midis[:5])

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic
import os, shutil, glob

RAW_INFER_DIR = "example_data/inference"
os.makedirs(RAW_INFER_DIR, exist_ok=True)

# Workspace mirror for you (like your local /Users/mihikadusad/MIDI_experiments)
MIDI_EXP_DIR = "/content/MIDI_experiments"
os.makedirs(MIDI_EXP_DIR, exist_ok=True)

# Use a subset of Kaggle MIDIs for inference (you can increase this)
all_midis = sorted(set(
    glob.glob("/content/kaggle_data/classical_midi/**/*.mid", recursive=True) +
    glob.glob("/content/kaggle_data/classical_midi/**/*.MID", recursive=True)
))
N_INFER = min(30, len(all_midis))
infer_midis = all_midis[:N_INFER]

for src in infer_midis:
    base = os.path.basename(src)
    shutil.copy(src, os.path.join(RAW_INFER_DIR, base))
    shutil.copy(src, os.path.join(MIDI_EXP_DIR, base))

print("example_data/train (built-in demo) contains:")
!ls example_data/train

print("\nexample_data/inference (Kaggle subset) contains:")
!ls example_data/inference | head

print("\nMIDI_experiments (mirror) contains:")
!ls /content/MIDI_experiments | head

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/muzic/getmusic

import os

PROCESSED_DIR = "example_data/processed_train"

# Hard reset: delete the entire processed_train directory
!rm -rf example_data/processed_train

# Recreate it empty
os.makedirs(PROCESSED_DIR, exist_ok=True)

print("Contents of example_data/processed_train BEFORE to_oct:")
!ls -la example_data/processed_train

print("\nRunning preprocess/to_oct.py ...\n")
!python preprocess/to_oct.py example_data/train example_data/processed_train | tee to_oct_log.txt

print("\nContents of example_data/processed_train AFTER to_oct:")
!ls -la example_data/processed_train

# Sanity check
if not os.path.exists("example_data/processed_train/oct.txt"):
    print("\n‚ùå oct.txt was NOT created. Last 40 lines of to_oct_log.txt:")
    !tail -n 40 to_oct_log.txt
else:
    print("\n‚úÖ oct.txt exists at example_data/processed_train/oct.txt")

!rm -rf example_data/processed_train