<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Music-Theory Informed Loss (Minimal)</title>
    <style>
      :root {
        --text: #000000;
        --muted: #4b5563;
        --border: #d1d5db;
        --accent: #000000;
        --bg: #f3f4f6;
      }

      * {
        box-sizing: border-box;
      }

      html {
        position: relative;
      }

      body {
        margin: 0;
        font-family: "Inter", -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
        color: var(--text);
        background: radial-gradient(circle at top left, #f9fafb 0%, #e5e7eb 40%, #d1d5db 100%);
        line-height: 1.7;
      }

      .page {
        max-width: 1200px;
        margin: 0 auto;
        padding: 40px 18px 80px;
        display: grid;
        grid-template-columns: 180px minmax(0, 1fr);
        gap: 28px;
        align-items: start;
      }

      main {
        padding: 0;
      }

      header {
        margin-bottom: 40px;
        border-bottom: 1px solid var(--border);
        padding-bottom: 16px;
        position: relative;
        overflow: hidden;
      }

      /* Floating musical notes confined to the header/title area */
      header::before {
        content: "♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬ ♪ ♫ ♩ ♬";
        position: absolute;
        top: 55%;
        left: -40%;
        width: 260%;
        pointer-events: none;
        font-size: 48px;
        color: rgba(0, 0, 0, 0.12);
        white-space: nowrap;
        z-index: -1;
        animation: float-notes-1 45s linear infinite;
      }

      @keyframes float-notes-1 {
        0% {
          transform: translateX(0);
        }
        100% {
          transform: translateX(40%);
        }
      }

      .kicker {
        text-transform: uppercase;
        letter-spacing: 0.12em;
        font-size: 12px;
        color: var(--muted);
        margin: 0 0 8px;
      }

      h1 {
        margin: 0 0 8px;
        font-size: 32px;
        line-height: 1.2;
        font-family: "Poppins", system-ui, -apple-system, BlinkMacSystemFont,
          "Helvetica Neue", Arial, sans-serif;
        letter-spacing: 0.03em;
      }

      .meta {
        margin: 0;
        color: var(--muted);
        font-size: 15px;
      }

      h2 {
        margin: 36px 0 12px;
        font-size: 18px; /* non-outline section headings (e.g., Training Progress) */
      }

      /* Larger style and music note icon for outline-linked sections */
      h2.toc-linked {
        font-size: 24px; /* key sections listed in the outline */
      }

      h2.toc-linked::before {
        content: "♪";
        display: inline-block;
        margin-right: 8px;
        font-size: 0.95em;
        color: #000000;
      }

      p {
        margin: 0 0 16px;
        font-size: 17px;
      }

      ul, ol {
        margin: 0 0 16px;
        padding-left: 24px;
      }

      li {
        margin-bottom: 8px;
        font-size: 17px;
      }

      img {
        display: block;
        margin: 0 auto;
        max-width: 100%;
        margin-bottom: 12px;
      }

      figure {
        margin: 24px 0;
      }

      figcaption {
        text-align: center;
        font-size: 14px;
        color: var(--muted);
        margin-top: 8px;
      }

      .figures-row {
        display: flex;
        gap: 1.5rem;
        justify-content: center;
        align-items: flex-start;
        margin: 24px 0;
        flex-wrap: wrap;
      }

      .figures-row figure {
        flex: 1 1 0;
        min-width: 280px;
        margin: 0;
      }

      @media (max-width: 640px) {
        .figures-row {
          flex-direction: column;
        }
      }

      .equation {
        margin: 16px 0;
        padding: 12px 14px;
        background: white;
        border: 1px solid var(--border);
        border-radius: 8px;
        overflow-x: auto;
      }

      a {
        color: var(--accent);
        text-decoration: none;
      }

      a:hover {
        text-decoration: underline;
      }

      sup a {
        font-size: 12px;
      }

      .references {
        margin-top: 48px;
        padding-top: 24px;
        border-top: 1px solid var(--border);
      }

      .references ol {
        margin: 12px 0 0 18px;
        padding: 0;
      }

      .references li {
        margin-bottom: 10px;
      }

      .toc,
      .sidenotes {
        position: sticky;
        top: 32px;
        align-self: start;
      }

      .toc {
        border: 1px solid var(--border);
        border-radius: 12px;
        padding: 16px 14px;
        background: rgba(255, 255, 255, 0.96);
        box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
      }

      .toc-title,
      .sidenote-title {
        margin: 0 0 10px;
        font-size: 13px;
        letter-spacing: 0.08em;
        text-transform: uppercase;
        color: var(--muted);
      }

      .toc ol {
        list-style: none;
        padding: 0;
        margin: 0;
      }

      .toc li {
        margin-bottom: 10px;
        font-size: 15px;
      }

      .sidenotes {
        padding: 12px 0;
      }

      .sidenotes ol {
        margin: 0;
        padding-left: 18px;
        color: var(--muted);
        font-size: 15px;
      }

      .sidenotes li {
        margin-bottom: 12px;
      }

      .sidenote-ref a {
        font-size: 12px;
        color: var(--muted);
      }

      @media (max-width: 1024px) {
        .page {
          grid-template-columns: 1fr;
          gap: 18px;
        }

        .toc,
        .sidenotes {
          position: relative;
          top: auto;
        }
      }

      @media (max-width: 640px) {
        .page {
          padding: 28px 14px 60px;
        }

        h1 {
          font-size: 26px;
        }

        h2 {
          font-size: 20px;
        }
      }
    </style>
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
        },
        options: {
          skipHtmlTags: [
            "script",
            "noscript",
            "style",
            "textarea",
            "pre",
            "code",
          ],
        },
      };
    </script>
    <script
      async
      id="MathJax-script"
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>
  <body>
    <div class="page">
      <aside class="toc" aria-label="Table of contents">
        <p class="toc-title">On this page</p>
        <nav>
          <ol>
            <li><a href="#introduction-title">Introduction</a></li>
            <li><a href="#background-title">Background &amp; Related Work</a></li>
            <li><a href="#methodology-title">Methodology &amp; Experiments</a></li>
            <li><a href="#results-title">Results &amp; Analysis</a></li>
            <li><a href="#conclusion-title">Conclusions</a></li>
            <li><a href="#references-title">References</a></li>
          </ol>
        </nav>
      </aside>

      <main class="content">
        <header>
          <p class="kicker">6.7960 Deep Learning Final Project</p>
          <h1>Music-Theory Informed Loss for Polyphonic MIDI Generation with Transformer Diffusion Models</h1>
          <p class="meta">
            <span style="font-weight:500;">Authors:</span>
            <a href="https://github.com/mihikadusad"><strong>Mihika Dusad</strong></a>
            ·
            <a href="https://github.com/victoriaxhu"><strong>Victoria Hu</strong></a>
          </p>
        </header>

        <section aria-labelledby="introduction-title">
          <h2 id="introduction-title" class="toc-linked">Introduction</h2>
          <p>
            AI-guided audio generation efforts have become increasingly popular, both in research and in industry [1, 2]. However, there is a key distinction between training models to generate pure audio and compose music. The process of composing true playable pieces, consisting of notes, rhythms, instruments, chords, is referred to as symbolic music generation. For symbolic music generation, models are generally trained to work with and generate MIDI (Musical Instrument Digital Interface) files, a digital score format that encodes musical events such as pitch, duration, timing, and instrument choice. Symbolic generation often focuses on polyphonic music, where multiple notes or musical lines sound simultaneously, such as in piano pieces, string quartets, or multi-track arrangements.
          </p>
          <p>
            Most current systems represent MIDIs as a single sequence of note events, forcing the model to implicitly discover 2D structure — melodic continuity, chord consistency, and voice independence — from flattened data. Despite improvements in tokenization schemes, these models often struggle to maintain stable voice lines or valid harmonic textures. In reality, polyphonic music is inherently two-dimensional: the horizontal axis reflects how melodies evolve over time, while the vertical axis captures the harmony formed by notes sounding together at each moment. Flattening this grid into a 1D sequence obscures both dimensions, making it significantly harder for models to learn coherent melodic motion and consistent chordal structure. One framework developed by Microsoft research, GETMusic [3], addresses this issue by converting MIDI inputs into a 2D track–time image-like representation, enabling a diffusion-based transformer to model local and global musical patterns directly in two dimensions. This representation introduces a stronger inductive bias than single-stream event models, since vertical and horizontal relationships are spatially explicit rather than reconstructed from a token sequence. However, harmony in GETMusic remains an emergent property: the diffusion objective is a generic reconstruction loss that does not encode any explicit music-theoretic principles. As a result, GETMusic remains heavily dependent on extremely large MIDI datasets, and the original authors invested substantial effort in crawling and cleaning thousands of hours of symbolic music to obtain sufficient training data.
          </p>
          <figure>
            <img src="./images/midi_demo.png" alt="MIDI demonstration showing 2D representation" />
            <figcaption><strong>Figure 1:</strong> MIDI demonstration showing piano roll representation. Note the distinction between audio waves and symbolic music. </figcaption>
          </figure>
          <p>
            We seek to build upon the GETMusic architecture by incorporating an explicit, music-theory-informed loss regularization term into the GETMusic framework to more reliably enforce vertical harmonic constraints. By aligning the objective function with known principles of polyphonic writing, we strengthen the inductive biases already present in GETMusic’s 2D representation and reduce the dependence on massive curated datasets. We aim to produce more coherent polyphonic outcomes, especially when training resources or datasets are limited.
          </p>
        </section>

        <section aria-labelledby="background-title">
          <h2 id="background-title" class="toc-linked">Background &amp; Related Work</h2>

          <h3>Sequence-based symbolic models</h3>
          <p>
            Early neural approaches to symbolic music generation modeled MIDI as a time-ordered sequence of events, typically using RNNs to generate monophonic or lightly polyphonic textures [4]. These models struggled to maintain long-range form, independent voices, and consistent harmonic context. When the transformer architecture was applied to music [5] for the first time in 2018, it introduced relative positional self-attention for long-range structure, but encoded all notes—across hands or tracks—as a single 1D event stream. Vertical relationships, i.e. which notes sound together at once, were implied only by co-occurring time indices in the sequence, not by a structural axis. Today, the transformer is the most commonly used architecture for MIDI generation, yet virtually all widely used architectures still rely on flattened 1D event streams, leaving multi-voice structure to be reconstructed implicitly by attention.
          </p>
          <p>
            Some other examples of modern symbolic music generators that interpret musical input as a single stream include:
          </p>
          <ul>
            <li>
              <strong>Compound Word Transformer (CP-Transformer)</strong> [6]: Groups attributes of a single note (pitch, duration, velocity, instrument) into compound tokens, shortening sequences and slightly tightening intra-note structure, but the representation remains a flat stream of notes.
            </li>
            <li>
              <strong>PopMAG / MuMIDI</strong> [7]: Proposes a multi-track sequence representation where track identity and chord information are encoded as token attributes. This allows multi-track accompaniment generation but further lengthens the sequence and still interleaves tracks in one stream.
            </li>
            <li>
              <strong>MMM (Multi-Track Music Machine), MTMT (Multitrack Music Transformer), and Nested Music Transformer</strong> [8–10]: Encode multi-track events as tuples (instrument, pitch, duration, etc.) that are then fully flattened into a 1D sequence.
            </li>
          </ul>
          <p>
            In these sequence-based models, harmony and voice-leading emerge from learned attention patterns over a long interleaved sequence. In practice, this creates well-known failure modes—voice crossing, unstable chord spacing, local dissonances, and loss of individual line identity—unless trained on a very large dataset and carefully regularized. Our work is motivated by this limitation: we aim to preserve the advantages of transformer-style modeling while moving towards representations and objectives that encode vertical structure more explicitly.
          </p>

          <h3>Image-based / 2D representations</h3>
          <p>
            A second family of methods abandons purely sequential representations and instead treats symbolic music as a 2D grid, usually a piano-roll with time on the horizontal axis and pitch on the vertical axis, sometimes with instrument tracks as channels.
          </p>
          <ul>
            <li>
              <strong>MuseGAN</strong> [11]: Generates multi-track piano-roll tensors, where each bar is represented as a binary matrix of size (time steps × pitches × tracks). Convolutional GANs operate directly on this 2D (plus channels) structure, so vertical chords and cross-track combinations are local patterns in an “image” rather than dispersed tokens, yielding strong local harmonic textures but typically limited to bar-level phrases with GAN training instabilities and weak global structure.
            </li>
            <li> (Yang et al., 2017)</strong> [12]: Uses convolutional GANs to generate melody one bar at a time in piano-roll form, conditioned on both 1D sequences (chords) and 2D contexts (previous bars). While the vertical axis explicitly encodes chord content, the objective is still a generic adversarial loss over images rather than a music-theoretic notion of harmony.
            </li>
            <li>
              <strong>DiffRoll</strong> [13]: Employs diffusion to generate piano-rolls conditioned on audio spectrograms for automatic music transcription. The model denoises a 2D time-pitch grid of symbolic events from Gaussian noise, demonstrating that diffusion in a piano-roll space can capture realistic local harmonic patterns.
            </li>
          </ul>

          <h3>GETMusic and 2D token grids</h3>
          <p>
            Architecturally, our work is closest to GETMusic [3], which positions itself between transformer sequence-based and image-based methods. GETMusic introduces GETScore, a 2D arrangement of discrete note tokens where tracks are stacked vertically and progress horizontally over time, with each track represented by two rows (pitch and duration). Compound pitch tokens merge simultaneous notes within a track, preserving intra-track chords while keeping the grid compact.
          </p>
          <p>
            A discrete diffusion model, GETDiff, denoises masked target tracks conditioned on source tracks in a non-autoregressive fashion, enabling flexible any-track-to-any-track generation and zero-shot infilling. Compared to piano-roll GANs/diffusion, GETScore avoids extremely large and sparse images while still making both horizontal continuity and vertical cross-track relationships spatially explicit. Compared to 1D event models such as PopMAG, it eliminates track interleaving: tracks are temporally aligned in a score-like grid rather than interwoven in a single stream.
          </p>
          <p>
            However, GETMusic’s objective remains a purely statistical discrete diffusion loss plus auxiliary reconstruction terms. There is no explicit bias towards music-theoretic principles beyond what is implicitly encoded in the dataset and the compound-token construction. In fact, the authors emphasize that they crawled and cleaned over 1.5M MuseScore MIDIs (around 2,800 hours of data) to train GETMusic, underscoring its dependence on very large curated datasets to learn harmonic structure robustly.
          </p>

          <h3>Positioning our contribution</h3>
          <p>
            Our project builds directly on this GETMusic line of work: we retain a 2D symbolic representation and a diffusion-style transformer backbone, but modify the learning objective by adding a music-theory-informed regularization term that explicitly evaluates the vertical slice (local harmony and spacing at each time step). Rather than treating harmony as a purely emergent property of data and generic denoising, we encode constraints derived from polyphonic writing practice—for example, penalties for certain dissonant intervals or unstable chord spacings, and rewards for consonant interval structures and consistent voice ranges—and integrate them into the loss function calculation.
          </p>
        </section>

        <section aria-labelledby="methodology-title">
          <h2 id="methodology-title" class="toc-linked">Methodology &amp; Experiments</h2>

          <p>
            To validate our hypothesis that explicit music-theoretic constraints can improve polyphonic generation—particularly in data-constrained environments—we implemented our proposed regularization framework on top of the GETMusic architecture. Our goal is to determine whether injecting domain-specific inductive biases into a diffusion-based symbolic music generator enables coherent harmonic structure even when trained on only a few hundred examples. We therefore conducted controlled experiments comparing the original GETMusic loss (GETM) against our proposed Music Theory Loss (MTL), using identical training conditions and evaluation metrics.
          </p>

          <h3>Dataset and Preprocessing</h3>
          <p>
            We downloaded the open source Slakh2100 dataset, which largely focuses on pop music. It contains 1500 training tracks, 375 validation tracks, and 225 test tracks. Our preprocessing pipeline consisted of three major stages:
          </p>

          <p>
            <strong>1. Track Merging.</strong>
            We mapped Slakh's multi-instrument arrangements into the standardized 14-row GETScore layout (7 instruments × 2 attributes: pitch and duration). GETMusic supports the following instruments, labeled by their instrument code: '0': piano, '25':guitar, '32':bass, '48':string, '80':lead melody; along with drums and chord labels.
          </p>

          <p>
            <strong>2. Filtering.</strong>
            Each MIDI file is converted into GETMusic's event-based OCT representation, where note events are assigned to a fixed grid of discrete time steps determined by the model's bar and positional resolution. This mapping ensures that notes, durations, and positions align with the model's multi-track format. After conversion, events that fall outside supported instrument types, have zero durations, or violate representation constraints are filtered out. The final encoded sequence is then chunked or padded to fit the model's maximum sequence length of 512 tokens for training.
          </p>

          <p>
            <strong>3. Data Pruning.</strong>
            Tracks that lacked sufficient polyphonic content or fell below a minimum length threshold were removed. The resulting curated dataset consisted of <strong>292 MIDI files</strong>—dramatically smaller than the 1.5M files used by the original GETMusic authors. This constrained setting is intentional: our goal is to test whether explicit harmonic regularization can compensate for limited data, a regime where standard diffusion-based systems often fail.
          </p>

          <h3>Architecture and the Music Theory Loss</h3>
          <p>
            Our backbone model is the Diffusion RoFormer (Rotary Transformer) used in the GETMusic framework. Let the logits over the discrete vocabulary at each diffusion step be denoted by
          </p>
          <div class="equation">
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mi>&ell;</mi>
                <mo>&#x2208;</mo>
                <msup>
                  <mi>&#x211D;</mi>
                  <mrow>
                    <mi>B</mi>
                    <mo>&#x00D7;</mo>
                    <mi>V</mi>
                    <mo>&#x00D7;</mo>
                    <mi>L</mi>
                  </mrow>
                </msup>
              </mrow>
            </math>
          </div>

          <p>
            where <em>B</em> is the batch size, <em>V</em> is the vocabulary size, and <em>L</em> is the sequence length in the GETScore grid. The corresponding conditional distribution over clean tokens at diffusion step <em>t</em> is
          </p>
          <div class="equation">
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mi>P</mi>
                <mo>(</mo>
                <msub>
                  <mi>x</mi>
                  <mn>0</mn>
                </msub>
                <mo>|</mo>
                <msub>
                  <mi>x</mi>
                  <mi>t</mi>
                </msub>
                <mo>)</mo>
                <mo>=</mo>
                <mi>softmax</mi>
                <mo>(</mo>
                <mi>&ell;</mi>
                <mo>)</mo>
                <mo>&#x2208;</mo>
                <msup>
                  <mi>&#x211D;</mi>
                  <mrow>
                    <mi>B</mi>
                    <mo>&#x00D7;</mo>
                    <mi>V</mi>
                    <mo>&#x00D7;</mo>
                    <mi>L</mi>
                  </mrow>
                </msup>
              </mrow>
            </math>
          </div>

          <p>
            The baseline GETMusic objective is the Variational Lower Bound (VLB) on the log-likelihood, which trains the model to reconstruct the clean token grid
            <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mn>0</mn></msub></math>
            from the corrupted state
            <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>t</mi></msub></math>.
            To encode music-theoretic structure into this process, we augment the VLB with our Music Theory Loss (MTL), obtaining the total training objective
          </p>
          <div class="equation">
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msub>
                  <mi>L</mi>
                  <mtext>total</mtext>
                </msub>
                <mo>=</mo>
                <msub>
                  <mi>L</mi>
                  <mtext>VLB</mtext>
                </msub>
                <mo>+</mo>
                <msub>
                  <mi>&#x03BB;</mi>
                  <mtext>MT</mtext>
                </msub>
                <msub>
                  <mi>L</mi>
                  <mtext>MT</mtext>
                </msub>
              </mrow>
            </math>
          </div>

          <p>
            Here,
            <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x03BB;</mi><mtext>MT</mtext></msub></math>
            controls the strength of the music-theoretic regularization. Conceptually, GETM refers to the baseline objective
            <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mtext>VLB</mtext></msub></math>,
            while MTL denotes our additional structured loss term
            <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mtext>MT</mtext></msub></math>.
          </p>

          <p>
            MTL is composed of two components: a Vertical Consonance Loss enforcing harmonic stability across tracks, and a Key Adherence Loss promoting tonal coherence.
          </p>

          <h4>Pitch-Class Distributions</h4>
          <p>
            We operate on pitch-class distributions aggregated from the vocabulary-level probabilities. Let
            <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&#x03C4;</mi></math>
            index time positions along the horizontal axis and <em>n</em> index tracks along the vertical axis. For each token position
            <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>(</mo><mi>&#x03C4;</mi><mo>,</mo><mi>n</mi><mo>)</mo></mrow></math>,
            we project from vocabulary space to the 12 pitch classes by summing over all vocabulary entries assigned to each pitch class via a fixed mapping matrix
            <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math>.
          </p>

          <p>
            This gives us a 12-dimensional distribution over pitch classes for each track and time step, which we use in both components of the Music Theory Loss.
          </p>
        </section>

        <section aria-labelledby="training-title">
          <h2 id="training-title">Training Progress</h2>
          <p>
            We trained both our modified architecture and the baseline MIDI generation model on a preprocessed collection of 292 MIDI files from slakh-2100, resulting in 1650 trainable segments of audio. We ran 50 epochs with 546 iterations per epoch. We also made use of a validation set during training. The following plots show the training and validation loss curves for both the baseline GETMusic model and our Music Theory Loss variant over the course of training.
          </p>
          <div class="figures-row">
            <figure>
              <img src="./images/baseline_loss_graph.png" alt="Training and Validation Loss per Epoch - Baseline Model">
              <figcaption><strong>Figure 2:</strong> Baseline GETMusic Model</figcaption>
            </figure>
            <figure>
              <img src="./images/modified_loss_graph.png" alt="Training and Validation Loss per Epoch - Music Theory Loss Model">
              <figcaption><strong>Figure 3:</strong> MTL Model</figcaption>
            </figure>
          </div>
          <p>
            As can be seen in Figures 2 and 3, the baseline GETMusic model converges much faster than the MTL model, which is likely due to an overfit on limited training data. 
          </p>
        </section>

        <section aria-labelledby="results-title">
          <h2 id="results-title" class="toc-linked">Results &amp; Analysis</h2>
          <p>
            In our experiments, both the baseline and modified models were prompted to generate a piano accompaniment track conditioned on the fixed lead-voice track. This setup isolates the effect of our changes on harmonic behavior, since the melodic line remains constant across models. We ran inference on 50 tracks from Slakh2100's cleaned test set.
          </p>
          <p>
            After generation, we ran an interval-analysis script that compares the piano part to the lead voice at each timestep and records the music-theoretic intervals formed between the two. This allowed us to quantify how often each interval class appears and whether the model shifts toward more consonant, stylistically appropriate harmonies.
          </p>
          <p>
            To visualize these differences, we computed frequency distributions of all intervals (unisons, thirds, fifths, etc.) for both the baseline and test models, included below.
          </p>
          <h3>Qualitative</h3>
          <p>
            We provide sample MIDIs that were generated by the original model and our modified model.
          </p>
          <p>
            <strong>Original loss:</strong><br />
            <audio controls>
              <source src="./images/baseline_sample_trim.wav" type="audio/wav">
              Your browser does not support the audio element.
            </audio>
          </p>
          <p>
            <strong>Music theory-based loss:</strong><br />
            <audio controls>
              <source src="./images/new_sample_trim.wav" type="audio/wav">
              Your browser does not support the audio element.
            </audio>
          </p>
          <p>
            In this example, we can see that the model using a music theory-based loss function does a slightly better job of generating a leading melody which maintains coherent vertical harmony. The music theory-based loss emphasizes the inclusion of more triads like the first G major chord you hear so clearly in the beginning. Some other stylistic differences can be discerned between the two recordings, such as the increased dissonance in the first recording and clashing of multiple tracks and instruments. Our loss function seems able to guide the model to prioritize inter-track harmony through the vertical consonance and key adherence loss components. However, given the sparsity of data and difficulty in objectively evaluating something as subjective asmusic, we turn to quantitative metrics.
          </p>

          <h3>Quantitative</h3>
          <p>
            We also wanted to quantitatively assess the differences in the model outputs. We ran an interval-analysis script that compares the piano part to the lead voice at each timestep and records the music-theoretic intervals formed between the two. This allowed us to quantify how often each interval class appears and whether the model shifts toward more consonant, stylistically appropriate harmonies.
          </p>
          <p>
            As can be seen in Figures 4 and 5, the music theory-based loss function creates a higher frequency of perfect intervals (unison, perfect fourth, perfect fifth) as well as major and minor thirds. On the other hand, the baseline generated a more diverse spread of intervals. This makes sense given our loss function; the key adherence loss in particular emphasizes maintaining regular intervals between adjacent notes.
          </p>
          <div class="figures-row">
            <figure>
              <img src="./images/interval_freq_baseline.png" alt="Interval frequency distribution - baseline model" />
              <figcaption><strong>Figure 4:</strong> Baseline model interval frequency distribution.</figcaption>
            </figure>
            <figure>
              <img src="./images/interval_freq_new.png" alt="Interval frequency distribution - music theory-based loss model" />
              <figcaption><strong>Figure 5:</strong> MTL model interval frequency distribution.</figcaption>
            </figure>
          </div>
          <p>
            We also looked at the distribution of key adherence per piece. We inferred the global key of a MIDI piece by selecting the major or minor key whose rotated Krumhansl–Schmuckler profile - a fixed distribution for the number of occurrences of each note in a specific key - best matched the piece's pitch-class histogram. Key adherence was then computed as the proportion of all notes whose pitch classes fall within the diatonic scale of that inferred key. As shown in Figures 6 and 7, we found that the key adherence was stronger in the baseline model rather than the MTL model. This was not an intended outcome.
          </p>
          <div class="figures-row">
            <figure>
              <img src="./images/key_adherence_baseline_ss.png" alt="Key adherence distribution - baseline model" />
              <figcaption><strong>Figure 6:</strong> Baseline model key adherence distribution.</figcaption>
            </figure>
            <figure>
              <img src="./images/key_adherence_new_ss.png" alt="Key adherence distribution - music theory-based loss model" />
              <figcaption><strong>Figure 7:</strong> MTL model key adherence distribution.</figcaption>
            </figure>
          </div>
        </section>

        <section aria-labelledby="conclusion-title">
          <h2 id="conclusion-title" class="toc-linked">Conclusions</h2>
          <p>
            Our preliminary experiments suggest that explicitly encoding simple harmonic priors into the loss can improve symbolic polyphonic generation in low-data settings, while remaining compatible with the existing GETMusic architecture.
          </p>
          <p>
            A key limitation of our current setup is the capacity/data mismatch: the architecture is quite powerful relative to the amount of high-quality symbolic data we were able to curate. In practice, scaling the dataset is non-trivial; finding open source data, cleaning MIDI files, and normalizing instrumentation proved to be some of the most technically challenging aspects of our process. Despite adding stronger inductive biases to partially compensate for limited data, these constraints still cap how reliably the model can internalize harmonic structure.
          </p>
          <p>
            Going forward, we see several directions. First, we plan to broaden the family of music-theory loss terms beyond the current key-adherence–style regularizer, including transposition-invariance losses that encourage similar predictions under global key shifts, discouraging extreme leaps, chord and progression losses based on functional harmony or circle-of-fifths proximity, as well as metrical and phrasing losses, such as downbeat chord stability, and preference for chord changes on strong beats. Second, our evaluation was constrained by the stylistic and structural limitations of the datasets we used. Because our training data focused on a narrow range of genres and track configurations, we were unable to exploit the rich diversity found in classical corpora such as Bach chorales, SATB a cappella scores, or multi-instrument orchestral datasets. Likewise, our testing primarily centered on a fixed subset of tracks, leaving rhythmic components—especially drums—underexplored. Broadening the stylistic scope and incorporating datasets with more varied harmonic and rhythmic profiles would likely improve the model's ability to generalize across musical textures.
          </p>
          <p>
            By systematically tuning these weights and progressively enriching the theory-aware loss portfolio—while also investing in larger, cleaner symbolic corpora—we expect future iterations to make more consistent use of musical structure, even under data-constrained settings.
          </p>
        </section>

        <section aria-labelledby="references-title" class="references">
          <h2 id="references-title" class="toc-linked">References</h2>
          <ol>
            <li>
              H. Wang, Y.-N. Chen, Z. Huang, et al.
              <em>MusicGen: Generating High-Fidelity Music from Text.</em>
              arXiv:2406.00146, 2024.
              Available at: <a href="https://arxiv.org/abs/2406.00146">https://arxiv.org/abs/2406.00146</a>.
            </li>
            <li>
              Suno AI.
              <em>Suno: Text-to-Music Generation Platform.</em>
              Available at: <a href="https://suno.com/home">https://suno.com/home</a>.
            </li>
            <li>
              Z. Lv, Z. Dai, G. Li, et al.
              <em>GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework.</em>
              arXiv:2305.10841, 2023.
            </li>
            <li>
              N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent.
              <em>
                Modeling Temporal Dependencies in High-Dimensional Sequences:
                Application to Polyphonic Music Generation and Transcription.
              </em>
              In <em>Proceedings of the 29th International Conference on Machine Learning (ICML)</em>, 2012.
              arXiv:1206.6392.
            </li>
            <li>
              C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, et al.
              <em>Music Transformer: Generating Music with Long-Term Structure.</em>
              In <em>International Conference on Learning Representations (ICLR)</em>, 2019.
              arXiv:1809.04281.
            </li>
            <li>
              W.-Y. Hsiao, J.-Y. Liu, Y.-H. Yang, and I. Liao.
              <em>Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs.</em>
              In <em>AAAI Conference on Artificial Intelligence</em>, 2021.
              arXiv:2101.02402.
            </li>
            <li>
              Y. Ren, J. He, X. Tan, et al.
              <em>PopMAG: Pop Music Accompaniment Generation.</em>
              In <em>Proceedings of the 28th ACM International Conference on Multimedia</em>, 2020.
              arXiv:2008.07703.
            </li>
            <li>
              J. Ens and P. Pasquier.
              <em>MMM: Exploring Conditional Multi-Track Music Generation with the Transformer.</em>
              In <em>Proceedings of the 21st International Society for Music Information Retrieval Conference (ISMIR)</em>, 2020.
              arXiv:2008.06048.
            </li>
            <li>
              H.-W. Dong, K. Chen, S. Dubnov, J. McAuley, and T. Berg-Kirkpatrick.
              <em>Multitrack Music Transformer: Learning Long-Term Dependencies in Music with Diverse Instruments.</em>
              In <em>ICASSP</em>, 2023.
              arXiv:2207.06983.
            </li>
            <li>
              K. Ryu, S. Cho, H. Choi, et al.
              <em>Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music.</em>
              In <em>Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)</em>, 2024.
              arXiv:2408.01180.
            </li>
            <li>
              H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang.
              <em>MuseGAN: Multi-Track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment.</em>
              In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2018.
              arXiv:1709.06298.
            </li>
            <li>
              L.-C. Yang, S.-Y. Chou, and Y.-H. Yang.
              <em>MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation.</em>
              In <em>Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)</em>, 2017.
              arXiv:1703.10847.
            </li>
            <li>
              K. W. Cheuk, R. Sawata, T. Uesaka, et al.
              <em>DiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability.</em>
              In <em>ICASSP</em>, 2023.
              arXiv:2210.05148.
            </li>
          </ol>
        </section>
      </main>

    </div>
  </body>
</html>
